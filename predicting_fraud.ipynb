{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bb9333",
   "metadata": {},
   "source": [
    "# Anomaly Detection: Flagging Credit Card Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfdc1dc",
   "metadata": {},
   "source": [
    "Anomaly detection is the practice of examining specific data points and detecting rare occurences that seem suspicious because they're different from the established pattern of behaviors. This can be a complex yet interesting problem to solve within the realm of data science. Assessing and attempting to predict credit card fraud transactions is an excellent opportunity to learn more about Anomaly Detection and discover some useful tips and processes for solutions to this issue. \n",
    "\n",
    "This project will cover:\n",
    "\n",
    "__1. Data Transformations__ \n",
    "\n",
    "__2. Feature Engineering__ \n",
    "\n",
    "__3. Handling Class Imbalance__\n",
    "\n",
    "__4. Choosing Baseline Models__ \n",
    "\n",
    "__5. Performance Metrics__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549168bb",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0936c7",
   "metadata": {},
   "source": [
    "First and foremost, we need to figure out what kind of problem we are trying to solve statistcally. With fraud detection an observation is classified as two things - Fraudulent or Not Fraudulent. This is a classification problem and therefore a [logistic regression](https://www.sciencedirect.com/topics/computer-science/logistic-regression#:~:text=Logistic%20regression%20is%20a%20process,%2Fno%2C%20and%20so%20on) problem. This is something to keep in mind as we build our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd7b47",
   "metadata": {},
   "source": [
    "Helpful Guide(s) for This Project: - https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_3_GettingStarted/BaselineFeatureTransformation.html\n",
    "                                   - https://machinelearningmastery.com/what-is-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4126cd",
   "metadata": {},
   "source": [
    "## Loading Data and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d9d80",
   "metadata": {},
   "source": [
    "The first order of business will be ensuring we have the necessary libraries and packages for our project. i did not have the imbalance-learn library so I had to download that for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec7045",
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#!pip install graphviz\n",
    "!pip install imbalanced-learn\n",
    "!pip install pandas\n",
    "!pip install math \n",
    "!pip install sys\n",
    "!pip install time\n",
    "!pip install pickle\n",
    "!pip install json\n",
    "!pip install datetime\n",
    "!pip intall random\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3292ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "#import sklearn\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# For imbalanced learning\n",
    "import imblearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbdd9b",
   "metadata": {},
   "source": [
    "Next, read in our data sets. Upon first glance, this dataset looks like a text file. After opening it, you'll see that it is formatted as a  __json schema__, so we'll use the read_json function available in pandas to read in our file and then take a look at our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3cea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('transactions.txt', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effc059",
   "metadata": {},
   "source": [
    "To get a better look at all the variables in our data set and get a sence of all of our data types to see if they make sense, we'll use dtypes function in pandas to take a look at all the columns in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51da0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accountNumber                 int64\n",
       "customerId                    int64\n",
       "creditLimit                   int64\n",
       "availableMoney              float64\n",
       "transactionDateTime          object\n",
       "transactionAmount           float64\n",
       "merchantName                 object\n",
       "acqCountry                   object\n",
       "merchantCountryCode          object\n",
       "posEntryMode                 object\n",
       "posConditionCode             object\n",
       "merchantCategoryCode         object\n",
       "currentExpDate               object\n",
       "accountOpenDate              object\n",
       "dateOfLastAddressChange      object\n",
       "cardCVV                       int64\n",
       "enteredCVV                    int64\n",
       "cardLast4Digits               int64\n",
       "transactionType              object\n",
       "echoBuffer                   object\n",
       "currentBalance              float64\n",
       "merchantCity                 object\n",
       "merchantState                object\n",
       "merchantZip                  object\n",
       "cardPresent                    bool\n",
       "posOnPremises                object\n",
       "recurringAuthInd             object\n",
       "expirationDateKeyInMatch       bool\n",
       "isFraud                        bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45d8c1",
   "metadata": {},
   "source": [
    "Lets take a look at some of our categories of interests to see their values, and their distribution. We'll first start of looking at our dependent varialble and true category of interest which is the __isFraud__ category. We have a total of 786363, almost 1 million transactions and 12,417 are fraudulant. This categorizes this dataset, as an imbalanced dataset since we don't have a very equitable distribution of Fraudulant and Genunine transaction values. We will deal with this issue later on in this project. Next lets look at our Customer, Credit Limit, and Transaction Amount distributions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc710854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of F/T: 2\n",
      "isFraud\n",
      "False    773946\n",
      "True      12417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of F/T:',df.groupby(['isFraud']).ngroups)\n",
    "print(df['isFraud'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729b95e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016043754990658264"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12417/773946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1cbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Customers: 5000\n",
      "Number of Unique Countries: 5\n",
      "merchantCountryCode\n",
      "US     778511\n",
      "MEX      3143\n",
      "CAN      2426\n",
      "PR       1559\n",
      "          724\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of Customers:',df.groupby(['accountNumber','customerId']).ngroups)\n",
    "print('Number of Unique Countries:',df.groupby(['merchantCountryCode']).ngroups)\n",
    "print(df['merchantCountryCode'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0bfa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Credit Limit: 250\n",
      "Min Transaction Amount: 0.0\n",
      "Max Credit Limit: 50000\n",
      "Max Transaction Amount: 2011.54\n"
     ]
    }
   ],
   "source": [
    "print('Min Credit Limit:',df['creditLimit'].min())\n",
    "print('Min Transaction Amount:',df['transactionAmount'].min())\n",
    "print('Max Credit Limit:',df['creditLimit'].max())\n",
    "print('Max Transaction Amount:',df['transactionAmount'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9a876",
   "metadata": {},
   "source": [
    "## Question 2: Plot\n",
    "Plot a histogram of the processed amounts of each transaction, the transactionAmount column.\n",
    "\n",
    "Report any structure you find and any hypotheses you have about that structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0eba7",
   "metadata": {},
   "source": [
    "## Question 3: Data Wrangling - Duplicate Transactions\n",
    "You will notice a number of what look like duplicated transactions in the data set. One type of duplicated transaction is a reversed transaction, where a purchase is followed by a reversal. Another example is a multi-swipe, where a vendor accidentally charges a customer's card multiple times within a short time span.\n",
    "\n",
    "Can you programmatically identify reversed and multi-swipe transactions?\n",
    "\n",
    "What total number of transactions and total dollar amount do you estimate for the reversed transactions? For the multi-swipe transactions? (please consider the first transaction to be \"normal\" and exclude it from the number of transaction and dollar amount counts)\n",
    "\n",
    "Did you find anything interesting about either kind of transaction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c0b09",
   "metadata": {},
   "source": [
    "## Question 4: Model\n",
    "Fraud is a problem for any bank. Fraud can take many forms, whether it is someone stealing a single credit card, to large batches of stolen credit card numbers being used on the web, or even a mass compromise of credit card numbers stolen from a merchant via tools like credit card skimming devices.\n",
    "\n",
    "Each of the transactions in the dataset has a field called isFraud. Please build a predictive model to determine whether a given transaction will be fraudulent or not. Use as much of the data as you like (or all of it).\n",
    "\n",
    "Provide an estimate of performance using an appropriate sample, and show your work.\n",
    "\n",
    "Please explain your methodology (modeling algorithm/method used and why, what features/data you found useful, what questions you have, and what you would do next with more time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24c273",
   "metadata": {},
   "source": [
    "## Data Preprocessing Steps\n",
    "Lets now do some analysis to preprocess our data, which essentially means we're going to look at our data and change some of the values in our dataset to make our dataset more digestible for the model we are going to create down the line. Some examples of this include:\n",
    "1. Encoding some of our categorical values (turning categorical values into integers)\n",
    "2. Creating other values based on other variables such as the datetime column \n",
    "3. Ensuring we have an appropriate index for our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648803ae",
   "metadata": {},
   "source": [
    "We'll first create an \"ID\" column to ensure we have a column that can act as an index for the unique amount of transactions we have in our dataset. None of our other variables ensure this so we'll create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3226169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ID\"] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f57412",
   "metadata": {},
   "source": [
    "Next we need to ensure all our boolean values such as the ones with *True*, *False* values are converted to integers. \n",
    "We also want to ensure our DateTime columns are formatted correctly. After applying the appropriate treatments for these values, lets check our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b57208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all our Boolean Values into integers\n",
    "df[['cardPresent','expirationDateKeyInMatch','isFraud']]=df[['cardPresent','expirationDateKeyInMatch','isFraud']].astype(int)\n",
    "#Convert all our dates to coorect format for Pandas\n",
    "df[['transactionDateTime','currentExpDate','accountOpenDate','dateOfLastAddressChange']] = df[['transactionDateTime','currentExpDate','accountOpenDate','dateOfLastAddressChange']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd221607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountNumber</th>\n",
       "      <th>customerId</th>\n",
       "      <th>creditLimit</th>\n",
       "      <th>availableMoney</th>\n",
       "      <th>transactionDateTime</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>merchantName</th>\n",
       "      <th>acqCountry</th>\n",
       "      <th>merchantCountryCode</th>\n",
       "      <th>posEntryMode</th>\n",
       "      <th>posConditionCode</th>\n",
       "      <th>merchantCategoryCode</th>\n",
       "      <th>currentExpDate</th>\n",
       "      <th>accountOpenDate</th>\n",
       "      <th>dateOfLastAddressChange</th>\n",
       "      <th>cardCVV</th>\n",
       "      <th>enteredCVV</th>\n",
       "      <th>cardLast4Digits</th>\n",
       "      <th>transactionType</th>\n",
       "      <th>echoBuffer</th>\n",
       "      <th>currentBalance</th>\n",
       "      <th>merchantCity</th>\n",
       "      <th>merchantState</th>\n",
       "      <th>merchantZip</th>\n",
       "      <th>cardPresent</th>\n",
       "      <th>posOnPremises</th>\n",
       "      <th>recurringAuthInd</th>\n",
       "      <th>expirationDateKeyInMatch</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>737265056</td>\n",
       "      <td>737265056</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-08-13 14:27:32</td>\n",
       "      <td>98.55</td>\n",
       "      <td>Uber</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>414</td>\n",
       "      <td>414</td>\n",
       "      <td>1803</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>737265056</td>\n",
       "      <td>737265056</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-10-11 05:05:54</td>\n",
       "      <td>74.51</td>\n",
       "      <td>AMC #191138</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>767</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>737265056</td>\n",
       "      <td>737265056</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-11-08 09:18:39</td>\n",
       "      <td>7.47</td>\n",
       "      <td>Play Store</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>mobileapps</td>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>767</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>737265056</td>\n",
       "      <td>737265056</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-12-10 02:14:50</td>\n",
       "      <td>7.47</td>\n",
       "      <td>Play Store</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>mobileapps</td>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>2015-03-14</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>767</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-03-24 21:04:46</td>\n",
       "      <td>71.18</td>\n",
       "      <td>Tim Hortons #947751</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2029-10-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-04-19 16:24:27</td>\n",
       "      <td>30.76</td>\n",
       "      <td>In-N-Out #422833</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-05-21 14:50:35</td>\n",
       "      <td>57.28</td>\n",
       "      <td>Krispy Kreme #685312</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-06-03 00:31:21</td>\n",
       "      <td>9.37</td>\n",
       "      <td>Shake Shack #968081</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>4990.63</td>\n",
       "      <td>2016-06-10 01:21:46</td>\n",
       "      <td>523.67</td>\n",
       "      <td>Burger King #486122</td>\n",
       "      <td></td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2032-08-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>9.37</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-07-11 10:47:16</td>\n",
       "      <td>164.37</td>\n",
       "      <td>Five Guys #510989</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>08</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-09-07 20:22:47</td>\n",
       "      <td>160.18</td>\n",
       "      <td>Auntie Anne's #747964</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>08</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-12-07 16:34:04</td>\n",
       "      <td>40.75</td>\n",
       "      <td>GreenCook</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>food_delivery</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>4959.25</td>\n",
       "      <td>2016-12-14 10:00:35</td>\n",
       "      <td>40.75</td>\n",
       "      <td>GreenCook</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>food_delivery</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>40.75</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>4918.50</td>\n",
       "      <td>2016-12-20 18:38:23</td>\n",
       "      <td>40.75</td>\n",
       "      <td>GreenCook</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>food_delivery</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>81.50</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>830329091</td>\n",
       "      <td>830329091</td>\n",
       "      <td>5000</td>\n",
       "      <td>4877.75</td>\n",
       "      <td>2016-12-28 06:43:01</td>\n",
       "      <td>40.75</td>\n",
       "      <td>GreenCook</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>food_delivery</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3143</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>122.25</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2016-01-02 11:19:46</td>\n",
       "      <td>30.08</td>\n",
       "      <td>Washington Repair</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2031-12-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>8522</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2469.92</td>\n",
       "      <td>2016-01-16 01:01:27</td>\n",
       "      <td>41.25</td>\n",
       "      <td>Eazy Tire</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2027-09-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>8522</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>30.08</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2428.67</td>\n",
       "      <td>2016-01-26 14:04:22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Convenient Auto Services</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>8522</td>\n",
       "      <td>ADDRESS_VERIFICATION</td>\n",
       "      <td></td>\n",
       "      <td>71.33</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2428.67</td>\n",
       "      <td>2016-01-29 07:17:39</td>\n",
       "      <td>124.21</td>\n",
       "      <td>Convenient Auto Services</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2032-08-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>579</td>\n",
       "      <td>579</td>\n",
       "      <td>4219</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>71.33</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2304.46</td>\n",
       "      <td>2016-01-29 07:33:15</td>\n",
       "      <td>196.07</td>\n",
       "      <td>Shell Auto Body</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>579</td>\n",
       "      <td>539</td>\n",
       "      <td>4219</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>195.54</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2108.39</td>\n",
       "      <td>2016-01-29 21:44:33</td>\n",
       "      <td>4.11</td>\n",
       "      <td>Convenient Auto Services</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2027-11-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>579</td>\n",
       "      <td>579</td>\n",
       "      <td>4219</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>391.61</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2016-02-06 08:16:46</td>\n",
       "      <td>108.86</td>\n",
       "      <td>Fast Auto Services</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2028-08-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>8522</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2391.14</td>\n",
       "      <td>2016-02-12 03:47:24</td>\n",
       "      <td>28.23</td>\n",
       "      <td>Washington Repair</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2032-09-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>579</td>\n",
       "      <td>579</td>\n",
       "      <td>4219</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>108.86</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2362.91</td>\n",
       "      <td>2016-02-22 17:32:13</td>\n",
       "      <td>26.17</td>\n",
       "      <td>Washington Repair</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2028-10-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>579</td>\n",
       "      <td>579</td>\n",
       "      <td>4219</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>137.09</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>574788567</td>\n",
       "      <td>574788567</td>\n",
       "      <td>2500</td>\n",
       "      <td>2336.74</td>\n",
       "      <td>2016-02-28 15:53:52</td>\n",
       "      <td>215.16</td>\n",
       "      <td>staples.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>8522</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>163.26</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accountNumber  customerId  creditLimit  availableMoney  \\\n",
       "0       737265056   737265056         5000         5000.00   \n",
       "1       737265056   737265056         5000         5000.00   \n",
       "2       737265056   737265056         5000         5000.00   \n",
       "3       737265056   737265056         5000         5000.00   \n",
       "4       830329091   830329091         5000         5000.00   \n",
       "5       830329091   830329091         5000         5000.00   \n",
       "6       830329091   830329091         5000         5000.00   \n",
       "7       830329091   830329091         5000         5000.00   \n",
       "8       830329091   830329091         5000         4990.63   \n",
       "9       830329091   830329091         5000         5000.00   \n",
       "10      830329091   830329091         5000         5000.00   \n",
       "11      830329091   830329091         5000         5000.00   \n",
       "12      830329091   830329091         5000         4959.25   \n",
       "13      830329091   830329091         5000         4918.50   \n",
       "14      830329091   830329091         5000         4877.75   \n",
       "15      574788567   574788567         2500         2500.00   \n",
       "16      574788567   574788567         2500         2469.92   \n",
       "17      574788567   574788567         2500         2428.67   \n",
       "18      574788567   574788567         2500         2428.67   \n",
       "19      574788567   574788567         2500         2304.46   \n",
       "20      574788567   574788567         2500         2108.39   \n",
       "21      574788567   574788567         2500         2500.00   \n",
       "22      574788567   574788567         2500         2391.14   \n",
       "23      574788567   574788567         2500         2362.91   \n",
       "24      574788567   574788567         2500         2336.74   \n",
       "\n",
       "   transactionDateTime  transactionAmount              merchantName  \\\n",
       "0  2016-08-13 14:27:32              98.55                      Uber   \n",
       "1  2016-10-11 05:05:54              74.51               AMC #191138   \n",
       "2  2016-11-08 09:18:39               7.47                Play Store   \n",
       "3  2016-12-10 02:14:50               7.47                Play Store   \n",
       "4  2016-03-24 21:04:46              71.18       Tim Hortons #947751   \n",
       "5  2016-04-19 16:24:27              30.76          In-N-Out #422833   \n",
       "6  2016-05-21 14:50:35              57.28      Krispy Kreme #685312   \n",
       "7  2016-06-03 00:31:21               9.37       Shake Shack #968081   \n",
       "8  2016-06-10 01:21:46             523.67       Burger King #486122   \n",
       "9  2016-07-11 10:47:16             164.37         Five Guys #510989   \n",
       "10 2016-09-07 20:22:47             160.18     Auntie Anne's #747964   \n",
       "11 2016-12-07 16:34:04              40.75                 GreenCook   \n",
       "12 2016-12-14 10:00:35              40.75                 GreenCook   \n",
       "13 2016-12-20 18:38:23              40.75                 GreenCook   \n",
       "14 2016-12-28 06:43:01              40.75                 GreenCook   \n",
       "15 2016-01-02 11:19:46              30.08         Washington Repair   \n",
       "16 2016-01-16 01:01:27              41.25                 Eazy Tire   \n",
       "17 2016-01-26 14:04:22               0.00  Convenient Auto Services   \n",
       "18 2016-01-29 07:17:39             124.21  Convenient Auto Services   \n",
       "19 2016-01-29 07:33:15             196.07           Shell Auto Body   \n",
       "20 2016-01-29 21:44:33               4.11  Convenient Auto Services   \n",
       "21 2016-02-06 08:16:46             108.86        Fast Auto Services   \n",
       "22 2016-02-12 03:47:24              28.23         Washington Repair   \n",
       "23 2016-02-22 17:32:13              26.17         Washington Repair   \n",
       "24 2016-02-28 15:53:52             215.16               staples.com   \n",
       "\n",
       "   acqCountry merchantCountryCode posEntryMode posConditionCode  \\\n",
       "0          US                  US           02               01   \n",
       "1          US                  US           09               01   \n",
       "2          US                  US           09               01   \n",
       "3          US                  US           09               01   \n",
       "4          US                  US           02               01   \n",
       "5          US                  US           02               01   \n",
       "6          US                  US           02               01   \n",
       "7          US                  US           05               01   \n",
       "8                              US           02               01   \n",
       "9          US                  US           05               08   \n",
       "10         US                  US           02               08   \n",
       "11         US                  US           09               01   \n",
       "12         US                  US           09               01   \n",
       "13         US                  US           09               01   \n",
       "14         US                  US           09               01   \n",
       "15         US                  US           02               01   \n",
       "16         US                  US           09               01   \n",
       "17         US                  US           09               01   \n",
       "18         US                  US           05               01   \n",
       "19         US                  US           09               01   \n",
       "20         US                  US           05               01   \n",
       "21         US                  US           09               01   \n",
       "22         US                  US           05               01   \n",
       "23         US                  US           02               01   \n",
       "24         US                  US           02               01   \n",
       "\n",
       "   merchantCategoryCode currentExpDate accountOpenDate  \\\n",
       "0             rideshare     2023-06-01      2015-03-14   \n",
       "1         entertainment     2024-02-01      2015-03-14   \n",
       "2            mobileapps     2025-08-01      2015-03-14   \n",
       "3            mobileapps     2025-08-01      2015-03-14   \n",
       "4              fastfood     2029-10-01      2015-08-06   \n",
       "5              fastfood     2020-01-01      2015-08-06   \n",
       "6              fastfood     2020-05-01      2015-08-06   \n",
       "7              fastfood     2021-01-01      2015-08-06   \n",
       "8              fastfood     2032-08-01      2015-08-06   \n",
       "9              fastfood     2020-04-01      2015-08-06   \n",
       "10             fastfood     2023-05-01      2015-08-06   \n",
       "11        food_delivery     2024-08-01      2015-08-06   \n",
       "12        food_delivery     2024-08-01      2015-08-06   \n",
       "13        food_delivery     2024-08-01      2015-08-06   \n",
       "14        food_delivery     2024-08-01      2015-08-06   \n",
       "15                 auto     2031-12-01      2015-10-13   \n",
       "16                 auto     2027-09-01      2015-10-13   \n",
       "17                 auto     2025-08-01      2015-10-13   \n",
       "18                 auto     2032-08-01      2015-10-13   \n",
       "19                 auto     2022-08-01      2015-10-13   \n",
       "20                 auto     2027-11-01      2015-10-13   \n",
       "21                 auto     2028-08-01      2015-10-13   \n",
       "22                 auto     2032-09-01      2015-10-13   \n",
       "23                 auto     2028-10-01      2015-10-13   \n",
       "24        online_retail     2021-12-01      2015-10-13   \n",
       "\n",
       "   dateOfLastAddressChange  cardCVV  enteredCVV  cardLast4Digits  \\\n",
       "0               2015-03-14      414         414             1803   \n",
       "1               2015-03-14      486         486              767   \n",
       "2               2015-03-14      486         486              767   \n",
       "3               2015-03-14      486         486              767   \n",
       "4               2015-08-06      885         885             3143   \n",
       "5               2015-08-06      885         885             3143   \n",
       "6               2015-08-06      885         885             3143   \n",
       "7               2015-08-06      885         885             3143   \n",
       "8               2015-08-06      885         885             3143   \n",
       "9               2015-08-06      885         885             3143   \n",
       "10              2015-08-06      885         885             3143   \n",
       "11              2015-08-06      885         885             3143   \n",
       "12              2015-08-06      885         885             3143   \n",
       "13              2015-08-06      885         885             3143   \n",
       "14              2015-08-06      885         885             3143   \n",
       "15              2015-10-13      206         206             8522   \n",
       "16              2015-10-13      206         206             8522   \n",
       "17              2015-10-13      206         206             8522   \n",
       "18              2015-10-13      579         579             4219   \n",
       "19              2015-10-13      579         539             4219   \n",
       "20              2015-10-13      579         579             4219   \n",
       "21              2015-10-13      206         206             8522   \n",
       "22              2015-10-13      579         579             4219   \n",
       "23              2015-10-13      579         579             4219   \n",
       "24              2015-10-13      206         206             8522   \n",
       "\n",
       "         transactionType echoBuffer  currentBalance merchantCity  \\\n",
       "0               PURCHASE                       0.00                \n",
       "1               PURCHASE                       0.00                \n",
       "2               PURCHASE                       0.00                \n",
       "3               PURCHASE                       0.00                \n",
       "4               PURCHASE                       0.00                \n",
       "5               PURCHASE                       0.00                \n",
       "6               PURCHASE                       0.00                \n",
       "7               PURCHASE                       0.00                \n",
       "8               PURCHASE                       9.37                \n",
       "9               PURCHASE                       0.00                \n",
       "10              PURCHASE                       0.00                \n",
       "11              PURCHASE                       0.00                \n",
       "12              PURCHASE                      40.75                \n",
       "13              PURCHASE                      81.50                \n",
       "14              PURCHASE                     122.25                \n",
       "15              PURCHASE                       0.00                \n",
       "16              PURCHASE                      30.08                \n",
       "17  ADDRESS_VERIFICATION                      71.33                \n",
       "18              PURCHASE                      71.33                \n",
       "19              PURCHASE                     195.54                \n",
       "20              PURCHASE                     391.61                \n",
       "21              PURCHASE                       0.00                \n",
       "22              PURCHASE                     108.86                \n",
       "23              PURCHASE                     137.09                \n",
       "24              PURCHASE                     163.26                \n",
       "\n",
       "   merchantState merchantZip  cardPresent posOnPremises recurringAuthInd  \\\n",
       "0                                       0                                  \n",
       "1                                       1                                  \n",
       "2                                       0                                  \n",
       "3                                       0                                  \n",
       "4                                       1                                  \n",
       "5                                       1                                  \n",
       "6                                       1                                  \n",
       "7                                       1                                  \n",
       "8                                       1                                  \n",
       "9                                       1                                  \n",
       "10                                      1                                  \n",
       "11                                      0                                  \n",
       "12                                      0                                  \n",
       "13                                      0                                  \n",
       "14                                      0                                  \n",
       "15                                      1                                  \n",
       "16                                      1                                  \n",
       "17                                      0                                  \n",
       "18                                      1                                  \n",
       "19                                      1                                  \n",
       "20                                      1                                  \n",
       "21                                      1                                  \n",
       "22                                      1                                  \n",
       "23                                      1                                  \n",
       "24                                      0                                  \n",
       "\n",
       "    expirationDateKeyInMatch  isFraud  ID  \n",
       "0                          0        0   1  \n",
       "1                          0        0   2  \n",
       "2                          0        0   3  \n",
       "3                          0        0   4  \n",
       "4                          0        0   5  \n",
       "5                          0        0   6  \n",
       "6                          0        0   7  \n",
       "7                          0        0   8  \n",
       "8                          0        0   9  \n",
       "9                          0        0  10  \n",
       "10                         0        0  11  \n",
       "11                         0        0  12  \n",
       "12                         0        0  13  \n",
       "13                         0        0  14  \n",
       "14                         0        0  15  \n",
       "15                         0        0  16  \n",
       "16                         0        0  17  \n",
       "17                         0        0  18  \n",
       "18                         0        0  19  \n",
       "19                         0        0  20  \n",
       "20                         0        0  21  \n",
       "21                         0        0  22  \n",
       "22                         0        0  23  \n",
       "23                         0        0  24  \n",
       "24                         0        0  25  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57621e5b",
   "metadata": {},
   "source": [
    "Now we want to create a column that tracks the elapsed time between our transactions, which will in effect group them into the day(s) our transactions occured.\n",
    "First we'll sort our transactions by date in ascending order to see the very first date of our first transactions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3d99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='transactionDateTime',ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd54547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountNumber</th>\n",
       "      <th>customerId</th>\n",
       "      <th>creditLimit</th>\n",
       "      <th>availableMoney</th>\n",
       "      <th>transactionDateTime</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>merchantName</th>\n",
       "      <th>acqCountry</th>\n",
       "      <th>merchantCountryCode</th>\n",
       "      <th>posEntryMode</th>\n",
       "      <th>posConditionCode</th>\n",
       "      <th>merchantCategoryCode</th>\n",
       "      <th>currentExpDate</th>\n",
       "      <th>accountOpenDate</th>\n",
       "      <th>dateOfLastAddressChange</th>\n",
       "      <th>cardCVV</th>\n",
       "      <th>enteredCVV</th>\n",
       "      <th>cardLast4Digits</th>\n",
       "      <th>transactionType</th>\n",
       "      <th>echoBuffer</th>\n",
       "      <th>currentBalance</th>\n",
       "      <th>merchantCity</th>\n",
       "      <th>merchantState</th>\n",
       "      <th>merchantZip</th>\n",
       "      <th>cardPresent</th>\n",
       "      <th>posOnPremises</th>\n",
       "      <th>recurringAuthInd</th>\n",
       "      <th>expirationDateKeyInMatch</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640789</th>\n",
       "      <td>419104777</td>\n",
       "      <td>419104777</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>2016-01-01 00:01:02</td>\n",
       "      <td>44.09</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>subscriptions</td>\n",
       "      <td>2028-03-01</td>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "      <td>5010</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28946</th>\n",
       "      <td>674577133</td>\n",
       "      <td>674577133</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:01:44</td>\n",
       "      <td>329.57</td>\n",
       "      <td>staples.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>08</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>1693</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222211</th>\n",
       "      <td>958438658</td>\n",
       "      <td>958438658</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>2016-01-01 00:01:47</td>\n",
       "      <td>164.57</td>\n",
       "      <td>cheapfast.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2013-07-20</td>\n",
       "      <td>2013-07-20</td>\n",
       "      <td>445</td>\n",
       "      <td>445</td>\n",
       "      <td>2062</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>222212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470320</th>\n",
       "      <td>851126461</td>\n",
       "      <td>851126461</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>2016-01-01 00:02:04</td>\n",
       "      <td>122.83</td>\n",
       "      <td>discount.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>08</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2025-07-01</td>\n",
       "      <td>2014-10-18</td>\n",
       "      <td>2014-10-18</td>\n",
       "      <td>667</td>\n",
       "      <td>667</td>\n",
       "      <td>7359</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>470321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704106</th>\n",
       "      <td>148963316</td>\n",
       "      <td>148963316</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2016-01-01 00:02:19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Fast Repair</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2026-12-01</td>\n",
       "      <td>2013-12-12</td>\n",
       "      <td>2013-12-12</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>1785</td>\n",
       "      <td>ADDRESS_VERIFICATION</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>704107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727644</th>\n",
       "      <td>974901832</td>\n",
       "      <td>974901832</td>\n",
       "      <td>250</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2016-01-01 00:03:47</td>\n",
       "      <td>24.56</td>\n",
       "      <td>staples.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2032-05-01</td>\n",
       "      <td>2012-05-29</td>\n",
       "      <td>2012-05-29</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>9744</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>727645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310263</th>\n",
       "      <td>811942128</td>\n",
       "      <td>811942128</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:04:10</td>\n",
       "      <td>20.45</td>\n",
       "      <td>sears.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2029-08-01</td>\n",
       "      <td>2015-05-23</td>\n",
       "      <td>2015-05-23</td>\n",
       "      <td>948</td>\n",
       "      <td>948</td>\n",
       "      <td>4888</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>310264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240190</th>\n",
       "      <td>380680241</td>\n",
       "      <td>380680241</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:06:17</td>\n",
       "      <td>96.68</td>\n",
       "      <td>Fresh Flowers</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>online_gifts</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>2014-06-21</td>\n",
       "      <td>2014-06-21</td>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "      <td>593</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305450</th>\n",
       "      <td>676919786</td>\n",
       "      <td>676919786</td>\n",
       "      <td>250</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2016-01-01 00:06:46</td>\n",
       "      <td>146.57</td>\n",
       "      <td>Dairy Queen #766986</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>3690</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>305451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622596</th>\n",
       "      <td>588383631</td>\n",
       "      <td>588383631</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:07:03</td>\n",
       "      <td>227.62</td>\n",
       "      <td>discount.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>2012-11-15</td>\n",
       "      <td>2012-11-15</td>\n",
       "      <td>792</td>\n",
       "      <td>792</td>\n",
       "      <td>557</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>622597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167079</th>\n",
       "      <td>225678947</td>\n",
       "      <td>225678947</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2016-01-01 00:09:07</td>\n",
       "      <td>44.73</td>\n",
       "      <td>Domino's Pizza #55020</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2026-09-01</td>\n",
       "      <td>2014-05-22</td>\n",
       "      <td>2014-05-22</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>9029</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632606</th>\n",
       "      <td>311956574</td>\n",
       "      <td>311956574</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>2016-01-01 00:09:23</td>\n",
       "      <td>42.61</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2012-08-07</td>\n",
       "      <td>2012-08-07</td>\n",
       "      <td>882</td>\n",
       "      <td>882</td>\n",
       "      <td>4133</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>632607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783960</th>\n",
       "      <td>473474510</td>\n",
       "      <td>473474510</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>2016-01-01 00:09:49</td>\n",
       "      <td>90.06</td>\n",
       "      <td>EZ Putt Putt #69446</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>2472</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>783961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186729</th>\n",
       "      <td>409650480</td>\n",
       "      <td>409650480</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:10:20</td>\n",
       "      <td>62.31</td>\n",
       "      <td>Uber</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>2031-04-01</td>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>5600</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445076</th>\n",
       "      <td>633329898</td>\n",
       "      <td>633329898</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>2016-01-01 00:10:29</td>\n",
       "      <td>15.94</td>\n",
       "      <td>In-N-Out #683809</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2008-05-25</td>\n",
       "      <td>2008-05-25</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>8266</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482395</th>\n",
       "      <td>682319827</td>\n",
       "      <td>682319827</td>\n",
       "      <td>7500</td>\n",
       "      <td>7500.00</td>\n",
       "      <td>2016-01-01 00:11:09</td>\n",
       "      <td>124.95</td>\n",
       "      <td>AMC #692956</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>2014-02-03</td>\n",
       "      <td>2014-02-03</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>2864</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>482396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692543</th>\n",
       "      <td>192849454</td>\n",
       "      <td>192849454</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>2016-01-01 00:12:02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>amazon.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2013-10-06</td>\n",
       "      <td>2013-10-06</td>\n",
       "      <td>840</td>\n",
       "      <td>840</td>\n",
       "      <td>9583</td>\n",
       "      <td>ADDRESS_VERIFICATION</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>692544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491910</th>\n",
       "      <td>846466728</td>\n",
       "      <td>846466728</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:12:20</td>\n",
       "      <td>144.51</td>\n",
       "      <td>ebay.com</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>08</td>\n",
       "      <td>online_retail</td>\n",
       "      <td>2028-04-01</td>\n",
       "      <td>2015-02-14</td>\n",
       "      <td>2015-02-14</td>\n",
       "      <td>776</td>\n",
       "      <td>776</td>\n",
       "      <td>3639</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>491911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27806</th>\n",
       "      <td>382001043</td>\n",
       "      <td>382001043</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>2016-01-01 00:12:44</td>\n",
       "      <td>101.49</td>\n",
       "      <td>Fast Tire</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>auto</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>2012-01-19</td>\n",
       "      <td>2012-01-19</td>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>2435</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475671</th>\n",
       "      <td>780024130</td>\n",
       "      <td>780024130</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:14:17</td>\n",
       "      <td>20.22</td>\n",
       "      <td>Blue Mountain Online Services</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>08</td>\n",
       "      <td>online_gifts</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>651</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>475672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240191</th>\n",
       "      <td>380680241</td>\n",
       "      <td>380680241</td>\n",
       "      <td>5000</td>\n",
       "      <td>4903.32</td>\n",
       "      <td>2016-01-01 00:14:21</td>\n",
       "      <td>339.52</td>\n",
       "      <td>Next Day eCards</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>online_gifts</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2014-06-21</td>\n",
       "      <td>2014-06-21</td>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "      <td>593</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>96.68</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705011</th>\n",
       "      <td>960197435</td>\n",
       "      <td>960197435</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2016-01-01 00:15:01</td>\n",
       "      <td>49.14</td>\n",
       "      <td>Uber</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>90</td>\n",
       "      <td>01</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>2030-12-01</td>\n",
       "      <td>2014-07-30</td>\n",
       "      <td>2014-07-30</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>3380</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>705012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579372</th>\n",
       "      <td>882815134</td>\n",
       "      <td>882815134</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>2016-01-01 00:15:48</td>\n",
       "      <td>6.73</td>\n",
       "      <td>Hilton Hotels #491145</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>09</td>\n",
       "      <td>08</td>\n",
       "      <td>hotels</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>959</td>\n",
       "      <td>959</td>\n",
       "      <td>8502</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>579373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678111</th>\n",
       "      <td>380083836</td>\n",
       "      <td>380083836</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>2016-01-01 00:16:01</td>\n",
       "      <td>55.33</td>\n",
       "      <td>In-N-Out #605436</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>738</td>\n",
       "      <td>738</td>\n",
       "      <td>7185</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>678112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700544</th>\n",
       "      <td>873724196</td>\n",
       "      <td>873724196</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>2016-01-01 00:16:03</td>\n",
       "      <td>32.06</td>\n",
       "      <td>NY Deli</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>02</td>\n",
       "      <td>08</td>\n",
       "      <td>food</td>\n",
       "      <td>2033-01-01</td>\n",
       "      <td>2015-09-29</td>\n",
       "      <td>2015-09-29</td>\n",
       "      <td>472</td>\n",
       "      <td>472</td>\n",
       "      <td>8792</td>\n",
       "      <td>PURCHASE</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accountNumber  customerId  creditLimit  availableMoney  \\\n",
       "640789      419104777   419104777        50000        50000.00   \n",
       "28946       674577133   674577133         5000         5000.00   \n",
       "222211      958438658   958438658        20000        20000.00   \n",
       "470320      851126461   851126461        10000        10000.00   \n",
       "704106      148963316   148963316         2500         2500.00   \n",
       "727644      974901832   974901832          250          250.00   \n",
       "310263      811942128   811942128         5000         5000.00   \n",
       "240190      380680241   380680241         5000         5000.00   \n",
       "305450      676919786   676919786          250          250.00   \n",
       "622596      588383631   588383631         5000         5000.00   \n",
       "167079      225678947   225678947         2500         2500.00   \n",
       "632606      311956574   311956574        50000        50000.00   \n",
       "783960      473474510   473474510        10000        10000.00   \n",
       "186729      409650480   409650480         5000         5000.00   \n",
       "445076      633329898   633329898        20000        20000.00   \n",
       "482395      682319827   682319827         7500         7500.00   \n",
       "692543      192849454   192849454        15000        15000.00   \n",
       "491910      846466728   846466728         5000         5000.00   \n",
       "27806       382001043   382001043        15000        15000.00   \n",
       "475671      780024130   780024130         5000         5000.00   \n",
       "240191      380680241   380680241         5000         4903.32   \n",
       "705011      960197435   960197435         2500         2500.00   \n",
       "579372      882815134   882815134        50000        50000.00   \n",
       "678111      380083836   380083836         5000         5000.00   \n",
       "700544      873724196   873724196        10000        10000.00   \n",
       "\n",
       "       transactionDateTime  transactionAmount                   merchantName  \\\n",
       "640789 2016-01-01 00:01:02              44.09                Washington Post   \n",
       "28946  2016-01-01 00:01:44             329.57                    staples.com   \n",
       "222211 2016-01-01 00:01:47             164.57                  cheapfast.com   \n",
       "470320 2016-01-01 00:02:04             122.83                   discount.com   \n",
       "704106 2016-01-01 00:02:19               0.00                    Fast Repair   \n",
       "727644 2016-01-01 00:03:47              24.56                    staples.com   \n",
       "310263 2016-01-01 00:04:10              20.45                      sears.com   \n",
       "240190 2016-01-01 00:06:17              96.68                  Fresh Flowers   \n",
       "305450 2016-01-01 00:06:46             146.57            Dairy Queen #766986   \n",
       "622596 2016-01-01 00:07:03             227.62                   discount.com   \n",
       "167079 2016-01-01 00:09:07              44.73          Domino's Pizza #55020   \n",
       "632606 2016-01-01 00:09:23              42.61                           Lyft   \n",
       "783960 2016-01-01 00:09:49              90.06            EZ Putt Putt #69446   \n",
       "186729 2016-01-01 00:10:20              62.31                           Uber   \n",
       "445076 2016-01-01 00:10:29              15.94               In-N-Out #683809   \n",
       "482395 2016-01-01 00:11:09             124.95                    AMC #692956   \n",
       "692543 2016-01-01 00:12:02               0.00                     amazon.com   \n",
       "491910 2016-01-01 00:12:20             144.51                       ebay.com   \n",
       "27806  2016-01-01 00:12:44             101.49                      Fast Tire   \n",
       "475671 2016-01-01 00:14:17              20.22  Blue Mountain Online Services   \n",
       "240191 2016-01-01 00:14:21             339.52                Next Day eCards   \n",
       "705011 2016-01-01 00:15:01              49.14                           Uber   \n",
       "579372 2016-01-01 00:15:48               6.73          Hilton Hotels #491145   \n",
       "678111 2016-01-01 00:16:01              55.33               In-N-Out #605436   \n",
       "700544 2016-01-01 00:16:03              32.06                        NY Deli   \n",
       "\n",
       "       acqCountry merchantCountryCode posEntryMode posConditionCode  \\\n",
       "640789         US                  US           09               01   \n",
       "28946          US                  US           09               08   \n",
       "222211         US                  US           05               01   \n",
       "470320         US                  US           02               08   \n",
       "704106         US                  US           05               01   \n",
       "727644         US                  US           05               01   \n",
       "310263         US                  US           02               01   \n",
       "240190         US                  US           05               01   \n",
       "305450         US                  US           05               01   \n",
       "622596         US                  US           02               01   \n",
       "167079         US                  US           05               01   \n",
       "632606         US                  US           05               01   \n",
       "783960         US                  US           02               01   \n",
       "186729         US                  US           05               01   \n",
       "445076         US                  US           05               01   \n",
       "482395         US                  US           09               01   \n",
       "692543         US                  US           05               01   \n",
       "491910         US                  US           02               08   \n",
       "27806          US                  US           09               01   \n",
       "475671         US                  US           05               08   \n",
       "240191         US                  US           02               01   \n",
       "705011         US                  US           90               01   \n",
       "579372         US                  US           09               08   \n",
       "678111         US                  US           05               01   \n",
       "700544         US                  US           02               08   \n",
       "\n",
       "       merchantCategoryCode currentExpDate accountOpenDate  \\\n",
       "640789        subscriptions     2028-03-01      2015-05-30   \n",
       "28946         online_retail     2024-10-01      2015-08-19   \n",
       "222211        online_retail     2023-04-01      2013-07-20   \n",
       "470320        online_retail     2025-07-01      2014-10-18   \n",
       "704106                 auto     2026-12-01      2013-12-12   \n",
       "727644        online_retail     2032-05-01      2012-05-29   \n",
       "310263        online_retail     2029-08-01      2015-05-23   \n",
       "240190         online_gifts     2023-08-01      2014-06-21   \n",
       "305450             fastfood     2020-12-01      2015-08-11   \n",
       "622596        online_retail     2021-09-01      2012-11-15   \n",
       "167079             fastfood     2026-09-01      2014-05-22   \n",
       "632606            rideshare     2024-08-01      2012-08-07   \n",
       "783960        entertainment     2025-11-01      2013-01-26   \n",
       "186729            rideshare     2031-04-01      2014-04-16   \n",
       "445076             fastfood     2024-01-01      2008-05-25   \n",
       "482395        entertainment     2020-11-01      2014-02-03   \n",
       "692543        online_retail     2024-03-01      2013-10-06   \n",
       "491910        online_retail     2028-04-01      2015-02-14   \n",
       "27806                  auto     2021-08-01      2012-01-19   \n",
       "475671         online_gifts     2025-01-01      2015-08-02   \n",
       "240191         online_gifts     2023-01-01      2014-06-21   \n",
       "705011            rideshare     2030-12-01      2014-07-30   \n",
       "579372               hotels     2020-03-01      2014-09-30   \n",
       "678111             fastfood     2021-04-01      2015-12-23   \n",
       "700544                 food     2033-01-01      2015-09-29   \n",
       "\n",
       "       dateOfLastAddressChange  cardCVV  enteredCVV  cardLast4Digits  \\\n",
       "640789              2015-05-30      837         837             5010   \n",
       "28946               2015-08-19      430         430             1693   \n",
       "222211              2013-07-20      445         445             2062   \n",
       "470320              2014-10-18      667         667             7359   \n",
       "704106              2013-12-12      542         542             1785   \n",
       "727644              2012-05-29      290         290             9744   \n",
       "310263              2015-05-23      948         948             4888   \n",
       "240190              2014-06-21      869         869              593   \n",
       "305450              2015-08-11      111         111             3690   \n",
       "622596              2012-11-15      792         792              557   \n",
       "167079              2014-05-22      889         889             9029   \n",
       "632606              2012-08-07      882         882             4133   \n",
       "783960              2013-01-26      496         496             2472   \n",
       "186729              2014-04-16      319         319             5600   \n",
       "445076              2008-05-25      490         490             8266   \n",
       "482395              2014-02-03      299         299             2864   \n",
       "692543              2013-10-06      840         840             9583   \n",
       "491910              2015-02-14      776         776             3639   \n",
       "27806               2012-01-19      723         723             2435   \n",
       "475671              2015-08-02      494         494              651   \n",
       "240191              2014-06-21      869         869              593   \n",
       "705011              2014-07-30      163         163             3380   \n",
       "579372              2014-09-30      959         959             8502   \n",
       "678111              2015-12-23      738         738             7185   \n",
       "700544              2015-09-29      472         472             8792   \n",
       "\n",
       "             transactionType echoBuffer  currentBalance merchantCity  \\\n",
       "640789              PURCHASE                       0.00                \n",
       "28946               PURCHASE                       0.00                \n",
       "222211              PURCHASE                       0.00                \n",
       "470320              PURCHASE                       0.00                \n",
       "704106  ADDRESS_VERIFICATION                       0.00                \n",
       "727644              PURCHASE                       0.00                \n",
       "310263              PURCHASE                       0.00                \n",
       "240190              PURCHASE                       0.00                \n",
       "305450              PURCHASE                       0.00                \n",
       "622596              PURCHASE                       0.00                \n",
       "167079              PURCHASE                       0.00                \n",
       "632606              PURCHASE                       0.00                \n",
       "783960              PURCHASE                       0.00                \n",
       "186729              PURCHASE                       0.00                \n",
       "445076              PURCHASE                       0.00                \n",
       "482395              PURCHASE                       0.00                \n",
       "692543  ADDRESS_VERIFICATION                       0.00                \n",
       "491910              PURCHASE                       0.00                \n",
       "27806               PURCHASE                       0.00                \n",
       "475671              PURCHASE                       0.00                \n",
       "240191              PURCHASE                      96.68                \n",
       "705011              PURCHASE                       0.00                \n",
       "579372              PURCHASE                       0.00                \n",
       "678111              PURCHASE                       0.00                \n",
       "700544              PURCHASE                       0.00                \n",
       "\n",
       "       merchantState merchantZip  cardPresent posOnPremises recurringAuthInd  \\\n",
       "640789                                      0                                  \n",
       "28946                                       0                                  \n",
       "222211                                      0                                  \n",
       "470320                                      0                                  \n",
       "704106                                      0                                  \n",
       "727644                                      0                                  \n",
       "310263                                      0                                  \n",
       "240190                                      0                                  \n",
       "305450                                      1                                  \n",
       "622596                                      0                                  \n",
       "167079                                      1                                  \n",
       "632606                                      0                                  \n",
       "783960                                      1                                  \n",
       "186729                                      0                                  \n",
       "445076                                      0                                  \n",
       "482395                                      1                                  \n",
       "692543                                      0                                  \n",
       "491910                                      0                                  \n",
       "27806                                       1                                  \n",
       "475671                                      0                                  \n",
       "240191                                      0                                  \n",
       "705011                                      0                                  \n",
       "579372                                      1                                  \n",
       "678111                                      1                                  \n",
       "700544                                      1                                  \n",
       "\n",
       "        expirationDateKeyInMatch  isFraud      ID  \n",
       "640789                         0        0  640790  \n",
       "28946                          0        0   28947  \n",
       "222211                         0        0  222212  \n",
       "470320                         0        0  470321  \n",
       "704106                         0        0  704107  \n",
       "727644                         0        0  727645  \n",
       "310263                         0        1  310264  \n",
       "240190                         0        0  240191  \n",
       "305450                         0        0  305451  \n",
       "622596                         0        0  622597  \n",
       "167079                         0        0  167080  \n",
       "632606                         0        0  632607  \n",
       "783960                         0        0  783961  \n",
       "186729                         0        0  186730  \n",
       "445076                         0        0  445077  \n",
       "482395                         0        0  482396  \n",
       "692543                         0        0  692544  \n",
       "491910                         0        0  491911  \n",
       "27806                          0        1   27807  \n",
       "475671                         0        0  475672  \n",
       "240191                         0        0  240192  \n",
       "705011                         0        0  705012  \n",
       "579372                         0        0  579373  \n",
       "678111                         0        0  678112  \n",
       "700544                         0        0  700545  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e46b3",
   "metadata": {},
   "source": [
    "Now that we can see the very first day of our transaction we can now use that first date as our basedate by which we will calculate the elapsed time of our transactions. \n",
    "We'll then create a new column named TX_Days tby which each observation will be calculated by subtracting the basedate of our dataset from the date of a specific transaction to calculate the elapsed time. After doing this we'll take a look at our dataframe to see our new column and its values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913177ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedate = pd.Timestamp('2016-01-01 00:01:02')\n",
    "df['TX_Days'] = (df['transactionDateTime'] - basedate).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75249203",
   "metadata": {},
   "source": [
    "Now lets take a look at our minimum transaction and our maximum transaction dates and days to get an idea of how many days of transaction data we have. We can see that we have about almost a years worth of transaction data to deal with in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min Transaction Time:',df['transactionDateTime'].min(),'|','First Day:',df['TX_Days'].min())\n",
    "print('Max Transaction Time:',df['transactionDateTime'].max(),'|','Last Day:',df['TX_Days'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c5b16",
   "metadata": {},
   "source": [
    "Next I'd like to impute the day of the week a specific transaction occurs into the dataframe (Monday, Tuesday...). This will be helpul later down the line for when we engineer some of our features for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'] = df['transactionDateTime'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee25a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc64b5a",
   "metadata": {},
   "source": [
    "Now lets take a look out our fraudulent transactions and take a look at some of their distributions. We'll put these values in a new data Frame called Fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud = df[df['isFraud'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of F/T:',Fraud.groupby(['isFraud']).ngroups)\n",
    "print(Fraud['isFraud'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9898c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weekend Numbers:',Fraud.groupby(['cardPresent']).ngroups)\n",
    "print(Fraud['cardPresent'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8631d",
   "metadata": {},
   "source": [
    "Now lets take a look at our total transactions in our data set through a line graph. We will plot the amount of total transactions that occur during the year with using our customer ID values as well as our TX_Days values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3479d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the stats we want\n",
    "def get_stats(df):\n",
    "    #Number of transactions per day\n",
    "    nb_tx_per_day=df.groupby(['TX_Days'])['customerId'].count()\n",
    "    #Number of fraudulent transactions per day\n",
    "    nb_fraud_per_day=df.groupby(['TX_Days'])['isFraud'].sum()\n",
    "    #Number of fraudulent cards per day\n",
    "    nb_fraudcard_per_day=df[df['isFraud']>0].groupby(['TX_Days']).customerId.nunique()\n",
    "    \n",
    "    return (nb_tx_per_day,nb_fraud_per_day,nb_fraudcard_per_day)\n",
    "\n",
    "(nb_tx_per_day,nb_fraud_per_day,nb_fraudcard_per_day)=get_stats(df)\n",
    "\n",
    "n_days=len(nb_tx_per_day)\n",
    "tx_stats=pd.DataFrame({\"value\":pd.concat([nb_tx_per_day/50,nb_fraud_per_day,nb_fraudcard_per_day])})\n",
    "tx_stats['stat_type']=[\"nb_tx_per_day\"]*n_days+[\"nb_fraud_per_day\"]*n_days+[\"nb_fraudcard_per_day\"]*n_days\n",
    "tx_stats=tx_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.4)\n",
    "\n",
    "fraud_and_transactions_stats_fig = plt.gcf()\n",
    "\n",
    "fraud_and_transactions_stats_fig.set_size_inches(15, 8)\n",
    "\n",
    "sns_plot = sns.lineplot(x=\"TX_Days\", y=\"value\", data=tx_stats, hue=\"stat_type\", hue_order=[\"nb_tx_per_day\",\"nb_fraud_per_day\",\"nb_fraudcard_per_day\"], legend=False)\n",
    "\n",
    "sns_plot.set_title('Total transactions, and number of fraudulent transactions \\n and number of compromised cards per day', fontsize=20)\n",
    "sns_plot.set(xlabel = \"Number of days since beginning of data generation\", ylabel=\"Number\")\n",
    "\n",
    "sns_plot.set_ylim([0,60])\n",
    "\n",
    "labels_legend = [\"# transactions per day (/50)\", \"# fraudulent txs per day\", \"# fraudulent cards per day\"]\n",
    "\n",
    "sns_plot.legend(loc='upper left', labels=labels_legend,bbox_to_anchor=(1.05, 1), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaf598",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our data and have gotten ready for some rigourous engineering, we will now pickle our data. As we saw before this is a very large data set with almost 1 million observations. Breaking up our data into weekly batches will assist us with analyzing our transactions in smaller chunks and pickling our data will allow us to easlily bring in our newly reformatted and prepocessed dataset without having to go through all of those steps we painstakingly took to get to this point. It will also help us load our data faster. \n",
    "We will create a new directory called \"pickled-data-raw\" and will direct all of our batched transaction data into this file. After running the code below, you should see a new folder in the folder this jupyter notebook lives in and within that folder you should see a list of pickle files with all our daily transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "\n",
    "if not os.path.exists(DIR_OUTPUT):\n",
    "    os.makedirs(DIR_OUTPUT)\n",
    "\n",
    "start_date = datetime.datetime.strptime(\"2016-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "for day in range(df.TX_Days.max()+1):\n",
    "    \n",
    "    transactions_day = df[df.TX_Days==day]\n",
    "    \n",
    "    date = start_date + datetime.timedelta(days=day)\n",
    "    filename_output = date.strftime(\"%Y-%m-%d\")+'.pkl'\n",
    "    \n",
    "    # Protocol=4 required for Google Colab\n",
    "    transactions_day.to_pickle(DIR_OUTPUT+filename_output, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a0600",
   "metadata": {},
   "source": [
    "We should have about 365 files in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a86c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(\"./pickled-data-raw/\")\n",
    "number_files = len(lst)\n",
    "print (number_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193d43b",
   "metadata": {},
   "source": [
    "## Feauture Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1853fce",
   "metadata": {},
   "source": [
    "Now its time to do some Feature Transformations. Before we do that, we have to create a function that will allow us to retrieve our data. Then we will load in some data to begin our feature transformations and to test out our read_from_files function. We'll load in data from **2016-06-25** to **2016-07-15** and use that as our dataset to create our new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter will be the directory, the begin date which and end date which will collect all the files with all those dates in between to append it into a dataframe\n",
    "def read_from_files(DIR_OUTPUT, begin_date, end_date):\n",
    "    #read in all the files in the directory\n",
    "    files = [os.path.join(DIR_OUTPUT, f) for f in os.listdir(DIR_OUTPUT) if f>=begin_date+'.pkl' and f<=end_date+'.pkl']\n",
    "    #create dataframes for each of of our files\n",
    "    frames = []\n",
    "    #create a forloop to read in the pickled files and append them\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf81b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "\n",
    "begin_date = '2016-06-25'\n",
    "end_date = '2016-07-15'\n",
    "\n",
    "df = read_from_files(DIR_OUTPUT, begin_date, end_date)\n",
    "print('{0} transactions loaded, containing {1} fraudulent transactions'.format(len(df), df.isFraud.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b42d4",
   "metadata": {},
   "source": [
    "### Features Engineered around Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdecf853",
   "metadata": {},
   "source": [
    "For our first transformation, we want to create a feature that assesses whether a transaction accured during the weekend or not. We will build a function that will return a transactiondate to us as a value of 0 if it happened during the weekday and 1 if it occured during the weekend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77403bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(transactionDateTime):\n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = transactionDateTime.weekday()\n",
    "    # Binary value: 0 if weekday, 1 if weekend\n",
    "    is_weekend = weekday>=5\n",
    "    \n",
    "    return int(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df['TX_DURING_WEEKEND']=df.transactionDateTime.apply(is_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad730b5",
   "metadata": {},
   "source": [
    "Next we'll also want to measure whether a transaction happened at night or not. In aquiring general knowledge about credit card fraud transactions, its been found that fraudulent transactions seem to be scewed toward the night time than other wise, so we'll create a function to measure that value and include it as possible feature in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a11362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_night(transactionDateTime):\n",
    "    \n",
    "    # Get the hour of the transaction\n",
    "    tx_hour = transactionDateTime.hour\n",
    "    # Binary value: 1 if hour less than 6, and 0 otherwise\n",
    "    is_night = tx_hour<=6\n",
    "    \n",
    "    return int(is_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9118dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df['TX_DURING_NIGHT']=df.transactionDateTime.apply(is_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1f438",
   "metadata": {},
   "source": [
    "### Features Engineered around Customer Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0709f",
   "metadata": {},
   "source": [
    "Next we'll create some features around the behavior of our customers. We want to see how customers spend their money within certain time intervals (1, 7, and 30 day windows). We want to see the total amount of transactions within that time frame as well as the average amount spent within that time frame too. We'll then take a look at our very first customer in our data frame to see what values our function will return. We'll do this with our *get_customer_spending_behaviour_features*. Our implementation relies on the Panda rolling function, which makes easy the computation of aggregates over a time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_spending_behaviour_features(customer_transactions, windows_size_in_days=[1,7,30]):\n",
    "    \n",
    "    # Let us first order transactions chronologically\n",
    "    customer_transactions=customer_transactions.sort_values('transactionDateTime')\n",
    "    \n",
    "    # The transaction date and time is set as the index, which will allow the use of the rolling function \n",
    "    customer_transactions.index=customer_transactions.transactionDateTime\n",
    "    \n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "        \n",
    "        # Compute the sum of the transaction amounts and the number of transactions for the given window size\n",
    "        SUM_AMOUNT_TX_WINDOW=customer_transactions['transactionAmount'].rolling(str(window_size)+'d').sum()\n",
    "        NB_TX_WINDOW=customer_transactions['transactionAmount'].rolling(str(window_size)+'d').count()\n",
    "    \n",
    "        # Compute the average transaction amount for the given window size\n",
    "        # NB_TX_WINDOW is always >0 since current transaction is always included\n",
    "        AVG_AMOUNT_TX_WINDOW=SUM_AMOUNT_TX_WINDOW/NB_TX_WINDOW\n",
    "    \n",
    "        # Save feature values\n",
    "        customer_transactions['CUSTOMER_ID_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        customer_transactions['CUSTOMER_ID_AVG_AMOUNT_'+str(window_size)+'DAY_WINDOW']=list(AVG_AMOUNT_TX_WINDOW)\n",
    "    \n",
    "    # Reindex according to transaction IDs\n",
    "    customer_transactions.index=customer_transactions.ID\n",
    "        \n",
    "    # And return the dataframe with the new features\n",
    "    return customer_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d73823",
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_behaviour_customer_1 = get_customer_spending_behaviour_features(df[df.ID==5])\n",
    "spending_behaviour_customer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46ce2c",
   "metadata": {},
   "source": [
    "Now that we see our function is working, we'll apply it to our whole dataframe and take a look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.groupby('customerId').apply(lambda x: get_customer_spending_behaviour_features(x, windows_size_in_days=[1,7,30]))\n",
    "df=df.sort_values('transactionDateTime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad381aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c154b8e",
   "metadata": {},
   "source": [
    "### Feature Engineered around Merchant Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c3604",
   "metadata": {},
   "source": [
    "Now we want to assess our merchant behaviors to see if there are any features we can engineer around them. The main goal of this will be to get a risk score, which will give us a sense of the exposure a specific merchant has to fraudulent transactions. The risk score will be calculated as the average number of fraudulent transactions that occured at store over a certain time window. \n",
    "\n",
    "With our customer ID transformations we used 3 window sizes of 1,7, and 30 days. But we will have to treat merchant transactions differently. The time windows will have to be shifted back by a delay period to account for the fact that in reality, transactions are discovered to be fraudulent after a fraud investigation or a customer complaint. As a result, fraudulent labels, which are needed for the risk score are only available after the delay period. This means the labels we need to compute the risk score in reality are only available after this delay period, so we'll account for that in our Merchant ID Transformations. The delay period will be set to about 1 week week (7 days).\n",
    "\n",
    "To get our risk score we'll have to define and create a *get_count_risk_rolling_window* function. The function takes as inputs the DataFrame of transactions for a given merchant, the delay period, and a list of window sizes(time intervals of 1,7,and 30 days). In the first stage, the number of transactions and fraudulent transactions are computed for the delay period (NB_TX_DELAY and NB_FRAUD_DELAY). In the second stage, the number of transactions and fraudulent transactions are computed for each window size plus the delay period (NB_TX_DELAY_WINDOW and NB_FRAUD_DELAY_WINDOW).  The number of transactions and fraudulent transactions that occurred for a given window size, shifted back by the delay period, is then obtained by simply computing the differences of the quantities obtained for the delay period, and the window size plus delay period.\n",
    "\n",
    "The risk score is then retrieved by computing the proportion of fraudulent transactions for each window size (or 0 if no transaction occurred for the given window)\n",
    "\n",
    "Our function will also return the number of transactions for each window size. This results in the addition of six new features: The risk and number of transactions, for three window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24945683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_risk_rolling_window(merchant_tx, delay_period=7, windows_size_in_days=[1,7,30], feature=\"merchantName\"):\n",
    "    \n",
    "    merchant_tx = merchant_tx.sort_values('transactionDateTime')\n",
    "    \n",
    "    merchant_tx.index=merchant_tx.transactionDateTime\n",
    "    \n",
    "    NB_FRAUD_DELAY=merchant_tx['isFraud'].rolling(str(delay_period)+'d').sum()\n",
    "    NB_TX_DELAY=merchant_tx['isFraud'].rolling(str(delay_period)+'d').count()\n",
    "    \n",
    "    for window_size in windows_size_in_days:\n",
    "    \n",
    "        NB_FRAUD_DELAY_WINDOW=merchant_tx['isFraud'].rolling(str(delay_period+window_size)+'d').sum()\n",
    "        NB_TX_DELAY_WINDOW=merchant_tx['isFraud'].rolling(str(delay_period+window_size)+'d').count()\n",
    "    \n",
    "        NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY\n",
    "        NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY\n",
    "    \n",
    "        RISK_WINDOW=NB_FRAUD_WINDOW/NB_TX_WINDOW\n",
    "        \n",
    "        merchant_tx[feature+'_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        merchant_tx[feature+'_RISK_'+str(window_size)+'DAY_WINDOW']=list(RISK_WINDOW)\n",
    "        \n",
    "    merchant_tx.index=merchant_tx.ID\n",
    "    \n",
    "    # Replace NA values with 0 (all undefined risk scores where NB_TX_WINDOW is 0) \n",
    "    merchant_tx.fillna(0,inplace=True)\n",
    "    \n",
    "    return merchant_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_count_risk_rolling_window(df[df.merchantName=='staples.com'], delay_period=7, windows_size_in_days=[1,7,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf243ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df=df.groupby('merchantName').apply(lambda x: get_count_risk_rolling_window(x, delay_period=7, windows_size_in_days=[1,7,30], feature=\"merchantName\"))\n",
    "df=df.sort_values('transactionDateTime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f3149",
   "metadata": {},
   "source": [
    "This wraps up our feature engineering and now we will save our data yet again into our pickle file directory so we can save our dataframe with all the new features we've included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136088d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "\n",
    "if not os.path.exists(DIR_OUTPUT):\n",
    "    os.makedirs(DIR_OUTPUT)\n",
    "\n",
    "start_date = datetime.datetime.strptime(\"2016-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "for day in range(df.TX_Days.max()+1):\n",
    "    \n",
    "    transactions_day = df[df.TX_Days==day]\n",
    "    \n",
    "    date = start_date + datetime.timedelta(days=day)\n",
    "    filename_output = date.strftime(\"%Y-%m-%d\")+'.pkl'\n",
    "    \n",
    "    # Protocol=4 required for Google Colab\n",
    "    transactions_day.to_pickle(DIR_OUTPUT+filename_output, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67f0f6",
   "metadata": {},
   "source": [
    "Just to double check, we'll load in our data with the time frames we used when we first loaded in our data to engineer our features to see if everything looks correct. We will name our dataframe df_check and look to see if all the new features we engineered are in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c265441",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "\n",
    "begin_date = '2016-06-25'\n",
    "end_date = '2016-07-15'\n",
    "\n",
    "df_check = read_from_files(DIR_OUTPUT, begin_date, end_date)\n",
    "print('{0} transactions loaded, containing {1} fraudulent transactions'.format(len(df_check), df_check.isFraud.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78720fa9",
   "metadata": {},
   "source": [
    "## Preliminary Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879e0d5d",
   "metadata": {},
   "source": [
    "Lets read in files from **2016-06-25** and **2016-07-25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_files(DIR_OUTPUT, begin_date, end_date):\n",
    "    #read in all the files in the directory\n",
    "    files = [os.path.join(DIR_OUTPUT, f) for f in os.listdir(DIR_OUTPUT) if f>=begin_date+'.pkl' and f<=end_date+'.pkl']\n",
    "    #create dataframes for each of of our files\n",
    "    frames = []\n",
    "    #create a forloop to read in the pickled files and append them\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "\n",
    "begin_date = '2016-06-25'\n",
    "end_date = '2016-07-25'\n",
    "\n",
    "df = read_from_files(DIR_OUTPUT, begin_date, end_date)\n",
    "print('{0} transactions loaded, containing {1} fraudulent transactions'.format(len(df), df.isFraud.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969a46c",
   "metadata": {},
   "source": [
    "Lets take a look at the number of transactions we have for each week. The first week of our dataset will be our training set, the next week will be the delay period, and the week after will be defined as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get a our dataframe for all the statistics we need for our line graph to assess our test, delay, and training sets\n",
    "def get_tx_stats(df, start_date_df = \"2016-01-01\"):\n",
    "    \n",
    "    #Number of transactions per day\n",
    "    nb_tx_per_day=df.groupby(['TX_Days'])['customerId'].count()\n",
    "    #Number of fraudulent transactions per day\n",
    "    nb_fraudulent_transactions_per_day=df.groupby(['TX_Days'])['isFraud'].sum()\n",
    "    #Number of compromised cards per day\n",
    "    nb_compromised_cards_per_day=df[df['isFraud']==1].groupby(['TX_Days']).customerId.nunique()\n",
    "    #Creating a new dataframe for our specific statistics\n",
    "    tx_stats=pd.DataFrame({\"nb_tx_per_day\":nb_tx_per_day,\n",
    "                           \"nb_fraudulent_transactions_per_day\":nb_fraudulent_transactions_per_day,\n",
    "                           \"nb_compromised_cards_per_day\":nb_compromised_cards_per_day})\n",
    "\n",
    "    tx_stats=tx_stats.reset_index()\n",
    "    \n",
    "    start_date = datetime.datetime.strptime(start_date_df, \"%Y-%m-%d\")\n",
    "    tx_date=start_date+tx_stats['TX_Days'].apply(datetime.timedelta)\n",
    "    \n",
    "    tx_stats['tx_date']=tx_date\n",
    "    \n",
    "    return tx_stats\n",
    "\n",
    "tx_stats=get_tx_stats(df, start_date_df=\"2016-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ab4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Plot the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "def get_template_tx_stats(ax ,fs,\n",
    "                          start_date_training,\n",
    "                          title='',\n",
    "                          delta_train=7,\n",
    "                          delta_delay=7,\n",
    "                          delta_test=7,\n",
    "                          ylim=100):\n",
    "    \n",
    "    ax.set_title(title, fontsize=fs*1.5)\n",
    "    ax.set_ylim([0, ylim])\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=fs)\n",
    "    ax.set_ylabel('Number', fontsize=fs)\n",
    "    \n",
    "    plt.yticks(fontsize=fs*0.7) \n",
    "    plt.xticks(fontsize=fs*0.7)    \n",
    "\n",
    "    ax.axvline(start_date_training+datetime.timedelta(days=delta_train), 0,ylim, color=\"black\")\n",
    "    ax.axvline(start_date_test, 0, ylim, color=\"black\")\n",
    "    \n",
    "    ax.text(start_date_training+datetime.timedelta(days=2), ylim-20,'Training period', fontsize=fs)\n",
    "    ax.text(start_date_training+datetime.timedelta(days=delta_train+2), ylim-20,'Delay period', fontsize=fs)\n",
    "    ax.text(start_date_training+datetime.timedelta(days=delta_train+delta_delay+2), ylim-20,'Test period', fontsize=fs)\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors={'nb_tx_per_day':cmap(0), \n",
    "        'nb_fraudulent_transactions_per_day':cmap(200), \n",
    "        'nb_compromised_cards_per_day':cmap(250)}\n",
    "\n",
    "fraud_and_transactions_stats_fig, ax = plt.subplots(1, 1, figsize=(15,8))\n",
    "\n",
    "# Training period\n",
    "start_date_training = datetime.datetime.strptime(\"2016-06-25\", \"%Y-%m-%d\")\n",
    "delta_train = delta_delay = delta_test = 7\n",
    "\n",
    "end_date_training = start_date_training+datetime.timedelta(days=delta_train-1)\n",
    "\n",
    "# Test period\n",
    "start_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay)\n",
    "end_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay+delta_test-1)\n",
    "\n",
    "get_template_tx_stats(ax, fs=20,\n",
    "                      start_date_training=start_date_training,\n",
    "                      title='Total transactions, and number of fraudulent transactions \\n and number of compromised cards per day',\n",
    "                      delta_train=delta_train,\n",
    "                      delta_delay=delta_delay,\n",
    "                      delta_test=delta_test\n",
    "                     )\n",
    "\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_tx_per_day']/50, 'b', color=colors['nb_tx_per_day'], label = '# transactions per day (/50)')\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_fraudulent_transactions_per_day'], 'b', color=colors['nb_fraudulent_transactions_per_day'], label = '# fraudulent txs per day')\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_compromised_cards_per_day'], 'b', color=colors['nb_compromised_cards_per_day'], label = '# compromised cards per day')\n",
    "\n",
    "ax.legend(loc = 'upper left',bbox_to_anchor=(1.05, 1),fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a754252",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_and_transactions_stats_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f821d1",
   "metadata": {},
   "source": [
    "Lets define our training and testings sets and take a look at both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75068d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_set(df,\n",
    "                       start_date_training,\n",
    "                       delta_train=7,delta_delay=7,delta_test=7):\n",
    "    \n",
    "    # Get the training set data\n",
    "    train_df = df[(df.transactionDateTime>=start_date_training) &\n",
    "                               (df.transactionDateTime<start_date_training+datetime.timedelta(days=delta_train))]\n",
    "    \n",
    "    # Get the test set data\n",
    "    test_df = []\n",
    "    \n",
    "    # Note: Cards known to be compromised after the delay period are removed from the test set\n",
    "    # That is, for each test day, all frauds known at (test_day-delay_period) are removed\n",
    "    \n",
    "    # First, get known defrauded customers from the training set\n",
    "    known_defrauded_customers = set(train_df[train_df.isFraud==1].customerId)\n",
    "    \n",
    "    # Get the relative starting day of training set (easier than TX_DATETIME to collect test data)\n",
    "    start_tx_time_days_training = train_df.TX_Days.min()\n",
    "    \n",
    "    # Then, for each day of the test set\n",
    "    for day in range(delta_test):\n",
    "    \n",
    "        # Get test data for that day\n",
    "        test_df_day = df[df.TX_Days==start_tx_time_days_training+ delta_train+delta_delay+day]\n",
    "        \n",
    "        # Compromised cards from that test day, minus the delay period, are added to the pool of known defrauded customers\n",
    "        test_df_day_delay_period = df[df.TX_Days==start_tx_time_days_training+\n",
    "                                                                                delta_train+\n",
    "                                                                                day-1]\n",
    "        \n",
    "        new_defrauded_customers = set(test_df_day_delay_period[test_df_day_delay_period.isFraud==1].customerId)\n",
    "        known_defrauded_customers = known_defrauded_customers.union(new_defrauded_customers)\n",
    "        \n",
    "        test_df_day = test_df_day[~test_df_day.customerId.isin(known_defrauded_customers)]\n",
    "        \n",
    "        test_df.append(test_df_day)\n",
    "        \n",
    "    test_df = pd.concat(test_df)\n",
    "    \n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df=train_df.sort_values('ID')\n",
    "    test_df=test_df.sort_values('ID')\n",
    "    \n",
    "    return (train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298500ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df, test_df)=get_train_test_set(df,start_date_training,\n",
    "                                       delta_train=7,delta_delay=7,delta_test=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc09156",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67456552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.isFraud==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df.isFraud==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "726/(45524+726)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2bbc5",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cafa84",
   "metadata": {},
   "source": [
    "We will first use Decision Trees as our first training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80615ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature=\"isFraud\"\n",
    "\n",
    "input_features=['transactionAmount','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'merchantName_NB_TX_1DAY_WINDOW',\n",
    "       'merchantName_RISK_1DAY_WINDOW', 'merchantName_NB_TX_7DAY_WINDOW',\n",
    "       'merchantName_RISK_7DAY_WINDOW', 'merchantName_NB_TX_30DAY_WINDOW',\n",
    "       'merchantName_RISK_30DAY_WINDOW','cardPresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_and_get_predictions(classifier, train_df, test_df, \n",
    "                                  input_features, output_feature=\"isFraud\",scale=True):\n",
    "    \n",
    "    # By default, scales input data\n",
    "    if scale:\n",
    "        (train_df, test_df)=scaleData(train_df,test_df,input_features)\n",
    "    \n",
    "    # We first train the classifier using the `fit` method, and pass as arguments the input and output features\n",
    "    start_time=time.time()\n",
    "    classifier.fit(train_df[input_features], train_df[output_feature])\n",
    "    training_execution_time=time.time()-start_time\n",
    "\n",
    "    # We then get the predictions on the training and test data using the `predict_proba` method\n",
    "    # The predictions are returned as a numpy array, that provides the probability of fraud for each transaction \n",
    "    start_time=time.time()\n",
    "    predictions_test=classifier.predict_proba(test_df[input_features])[:,1]\n",
    "    prediction_execution_time=time.time()-start_time\n",
    "    \n",
    "    predictions_train=classifier.predict_proba(train_df[input_features])[:,1]\n",
    "\n",
    "    # The result is returned as a dictionary containing the fitted models, \n",
    "    # and the predictions on the training and test sets\n",
    "    model_and_predictions_dictionary = {'classifier': classifier,\n",
    "                                        'predictions_test': predictions_test,\n",
    "                                        'predictions_train': predictions_train,\n",
    "                                        'training_execution_time': training_execution_time,\n",
    "                                        'prediction_execution_time': prediction_execution_time\n",
    "                                       }\n",
    "    \n",
    "    return model_and_predictions_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.tree.DecisionTreeClassifier(max_depth = 2, random_state=0)\n",
    "\n",
    "model_and_predictions_dictionary = fit_model_and_get_predictions(classifier, train_df, test_df, \n",
    "                                                                 input_features, output_feature,\n",
    "                                                                 scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['TX_FRAUD_PREDICTED']=model_and_predictions_dictionary['predictions_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf97e3",
   "metadata": {},
   "source": [
    "The probability of fraud for all these transactions is of 0.015920. We can display the decision tree to understand how these probabilities were set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing decision tree with maplotlib. Graphviz is another option\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "\n",
    "tree.plot_tree(classifier,\n",
    "           feature_names = input_features, \n",
    "           class_names=True,\n",
    "           filled = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_day(df_day,top_k):\n",
    "    \n",
    "    # This takes the max of the predictions AND the max of label TX_FRAUD for each CUSTOMER_ID, \n",
    "    # and sorts by decreasing order of fraudulent prediction\n",
    "    df_day = df_day.groupby('customerId').max().sort_values(by=\"predictions\", ascending=False).reset_index(drop=False)\n",
    "            \n",
    "    # Get the top k most suspicious cards\n",
    "    df_day_top_k=df_day.head(top_k)\n",
    "    list_detected_compromised_cards=list(df_day_top_k[df_day_top_k.isFraud==1].customerId)\n",
    "    \n",
    "    # Compute precision top k\n",
    "    card_precision_top_k = len(list_detected_compromised_cards) / top_k\n",
    "    \n",
    "    return list_detected_compromised_cards, card_precision_top_k\n",
    "\n",
    "def card_precision_top_k(predictions_df, top_k, remove_detected_compromised_cards=True):\n",
    "\n",
    "    # Sort days by increasing order\n",
    "    list_days=list(predictions_df['TX_Days'].unique())\n",
    "    list_days.sort()\n",
    "    \n",
    "    # At first, the list of detected compromised cards is empty\n",
    "    list_detected_compromised_cards = []\n",
    "    \n",
    "    card_precision_top_k_per_day_list = []\n",
    "    nb_compromised_cards_per_day = []\n",
    "    \n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "        \n",
    "        df_day = predictions_df[predictions_df['TX_Days']==day]\n",
    "        df_day = df_day[['predictions', 'customerId', 'isFraud']]\n",
    "        \n",
    "        # Let us remove detected compromised cards from the set of daily transactions\n",
    "        df_day = df_day[df_day.customerId.isin(list_detected_compromised_cards)==False]\n",
    "        \n",
    "        nb_compromised_cards_per_day.append(len(df_day[df_day.isFraud==1].customerId.unique()))\n",
    "        \n",
    "        detected_compromised_cards, card_precision_top_k = card_precision_top_k_day(df_day,top_k)\n",
    "        \n",
    "        card_precision_top_k_per_day_list.append(card_precision_top_k)\n",
    "        \n",
    "        # Let us update the list of detected compromised cards\n",
    "        if remove_detected_compromised_cards:\n",
    "            list_detected_compromised_cards.extend(detected_compromised_cards)\n",
    "        \n",
    "    # Compute the mean\n",
    "    mean_card_precision_top_k = np.array(card_precision_top_k_per_day_list).mean()\n",
    "    \n",
    "    # Returns precision top k per day as a list, and resulting mean\n",
    "    return nb_compromised_cards_per_day,card_precision_top_k_per_day_list,mean_card_precision_top_k\n",
    "\n",
    "def performance_assessment(predictions_df, output_feature='isFraud', \n",
    "                           prediction_feature='predictions', top_k_list=[100],\n",
    "                           rounded=True):\n",
    "    \n",
    "    AUC_ROC = metrics.roc_auc_score(predictions_df[output_feature], predictions_df[prediction_feature])\n",
    "    AP = metrics.average_precision_score(predictions_df[output_feature], predictions_df[prediction_feature])\n",
    "    \n",
    "    performances = pd.DataFrame([[AUC_ROC, AP]], \n",
    "                           columns=['AUC ROC','Average precision'])\n",
    "    \n",
    "    for top_k in top_k_list:\n",
    "    \n",
    "        _, _, mean_card_precision_top_k = card_precision_top_k(predictions_df, top_k)\n",
    "        performances['Card Precision@'+str(top_k)]=mean_card_precision_top_k\n",
    "        \n",
    "    if rounded:\n",
    "        performances = performances.round(3)\n",
    "    \n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc2016",
   "metadata": {},
   "source": [
    "The Card Precision top-k is the most pragmatic and interpretable measure. It takes into account the fact that investigators can only check a maximum of potentially fraudulent cards per day. It is computed by ranking, for every day in the test set, the most fraudulent transactions, and selecting the cards whose transactions have the highest fraud probabilities. The precision (proportion of actual compromised cards out of predicted compromised cards) is then computed for each day. The Card Precision top-k is the average of these daily precisions. The number will be set to 100 (that is, it is assumed that only 100 cards can be checked every day). The metric is described in detail in Chapter 4, Precision top-k metrics.\n",
    "\n",
    "The Average Precision is a proxy for the Card Precision top-k, that integrates precisions for all possible values\n",
    "\n",
    "The AUC ROC is an alternative measure to the Average Precision, which gives more importance to scores obtained with higher values. It is less relevant in practice since the performances that matter most are those for low values. We however also report it since it is the most widely used performance metric for fraud detection in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df=test_df\n",
    "predictions_df['predictions']=model_and_predictions_dictionary['predictions_test']\n",
    "    \n",
    "performance_assessment(predictions_df, top_k_list=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7684ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['predictions']=0.5\n",
    "    \n",
    "performance_assessment(predictions_df, top_k_list=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35291e76",
   "metadata": {},
   "source": [
    "### Utilizing Standard Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbb59f",
   "metadata": {},
   "source": [
    "Lets now train our models on standard predictive models such Logistic Regression, Random Forest, XGBoost, as well as other iterations of Decision Trees. For this purpose, we will create a dictionary of sklearn classifiers that instantiates each of these classifiers. We then train and compute the predictions for each of these classifiers using the fit_model_and_get_predictions function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(train,test,features):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(train[features])\n",
    "    train[features]=scaler.transform(train[features])\n",
    "    test[features]=scaler.transform(test[features])\n",
    "    \n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_dictionary={'Logistic regression':sklearn.linear_model.LogisticRegression(random_state=0), \n",
    "                        'Decision tree with depth of two':sklearn.tree.DecisionTreeClassifier(max_depth=2,random_state=0), \n",
    "                        'Decision tree - unlimited depth':sklearn.tree.DecisionTreeClassifier(random_state=0), \n",
    "                        'Random forest':sklearn.ensemble.RandomForestClassifier(random_state=0,n_jobs=-1),\n",
    "                        'XGBoost':xgboost.XGBClassifier(random_state=0,n_jobs=-1),\n",
    "                       }\n",
    "\n",
    "fitted_models_and_predictions_dictionary={}\n",
    "\n",
    "for classifier_name in classifiers_dictionary:\n",
    "    \n",
    "    model_and_predictions = fit_model_and_get_predictions(classifiers_dictionary[classifier_name], train_df, test_df, \n",
    "                                                                                  input_features=input_features,\n",
    "                                                                                output_feature=output_feature)\n",
    "    fitted_models_and_predictions_dictionary[classifier_name]=model_and_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_assessment_model_collection(fitted_models_and_predictions_dictionary, \n",
    "                                            df, \n",
    "                                            type_set='test',\n",
    "                                            top_k_list=[100]):\n",
    "\n",
    "    performances=pd.DataFrame() \n",
    "    \n",
    "    for classifier_name, model_and_predictions in fitted_models_and_predictions_dictionary.items():\n",
    "    \n",
    "        predictions_df= df\n",
    "            \n",
    "        predictions_df['predictions']=model_and_predictions['predictions_'+type_set]\n",
    "        \n",
    "        performances_model=performance_assessment(predictions_df, output_feature='isFraud', \n",
    "                                                   prediction_feature='predictions', top_k_list=top_k_list)\n",
    "        performances_model.index=[classifier_name]\n",
    "        \n",
    "        performances=performances.append(performances_model)\n",
    "        \n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779540d9",
   "metadata": {},
   "source": [
    "Lets take a look at the performances of this model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04519c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performances on test set\n",
    "df_performances=performance_assessment_model_collection(fitted_models_and_predictions_dictionary, test_df, \n",
    "                                                        type_set='test', \n",
    "                                                        top_k_list=[100])\n",
    "df_performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d701063",
   "metadata": {},
   "source": [
    "Lets take a look at the performances of this model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddeac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performances on training set\n",
    "df_performances=performance_assessment_model_collection(fitted_models_and_predictions_dictionary, train_df, \n",
    "                                                        type_set='train', \n",
    "                                                        top_k_list=[100])\n",
    "df_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83087aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_times_model_collection(fitted_models_and_predictions_dictionary):\n",
    "\n",
    "    execution_times=pd.DataFrame() \n",
    "    \n",
    "    for classifier_name, model_and_predictions in fitted_models_and_predictions_dictionary.items():\n",
    "    \n",
    "        execution_times_model=pd.DataFrame() \n",
    "        execution_times_model['Training execution time']=[model_and_predictions['training_execution_time']]\n",
    "        execution_times_model['Prediction execution time']=[model_and_predictions['prediction_execution_time']]\n",
    "        execution_times_model.index=[classifier_name]\n",
    "        \n",
    "        execution_times=execution_times.append(execution_times_model)\n",
    "        \n",
    "    return execution_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afec666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution times\n",
    "df_execution_times=execution_times_model_collection(fitted_models_and_predictions_dictionary)\n",
    "df_execution_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e064faf",
   "metadata": {},
   "source": [
    "### Plot AUC ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4676c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "begin_date = '2016-06-25'\n",
    "end_date = '2016-07-25'\n",
    "\n",
    "%time df=read_from_files(DIR_OUTPUT, begin_date, end_date)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(df),df.isFraud.sum()))\n",
    "\n",
    "start_date_training = datetime.datetime.strptime(\"2016-06-25\", \"%Y-%m-%d\")\n",
    "delta_train=delta_delay=delta_test=7\n",
    "\n",
    "(train_df,test_df)=get_train_test_set(df,start_date_training,\n",
    "                                      delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "output_feature=\"isFraud\"\n",
    "\n",
    "input_features=['transactionAmount','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'merchantName_NB_TX_1DAY_WINDOW',\n",
    "       'merchantName_RISK_1DAY_WINDOW', 'merchantName_NB_TX_7DAY_WINDOW',\n",
    "       'merchantName_RISK_7DAY_WINDOW', 'merchantName_NB_TX_30DAY_WINDOW',\n",
    "       'merchantName_RISK_30DAY_WINDOW','cardPresent']\n",
    "\n",
    "classifiers_dictionary={'Logistic regression':sklearn.linear_model.LogisticRegression(random_state=0), \n",
    "                        'Decision tree with depth of two':sklearn.tree.DecisionTreeClassifier(max_depth=2,random_state=0), \n",
    "                        'Decision tree - unlimited depth':sklearn.tree.DecisionTreeClassifier(random_state=0), \n",
    "                        'Random forest':sklearn.ensemble.RandomForestClassifier(random_state=0,n_jobs=-1),\n",
    "                        'XGBoost':xgboost.XGBClassifier(random_state=0,n_jobs=-1),\n",
    "                       }\n",
    "\n",
    "fitted_models_and_predictions_dictionary={}\n",
    "\n",
    "for classifier_name in classifiers_dictionary:\n",
    "    \n",
    "    start_time=time.time()\n",
    "    model_and_predictions = fit_model_and_get_predictions(classifiers_dictionary[classifier_name], train_df, test_df, \n",
    "                                                          input_features=input_features,\n",
    "                                                          output_feature=output_feature)\n",
    "    \n",
    "    print(\"Time to fit the \"+classifier_name+\" model: \"+str(round(time.time()-start_time,2)))\n",
    "    \n",
    "    fitted_models_and_predictions_dictionary[classifier_name]=model_and_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add98e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.isFraud==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a663dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_roc_curve(ax, title,fs,random=True):\n",
    "    \n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=fs)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=fs)\n",
    "    \n",
    "    if random:\n",
    "        ax.plot([0, 1], [0, 1],'r--',label=\"AUC ROC Random = 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "roc_curve, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors={'Logistic regression':cmap(0), 'Decision tree with depth of two':cmap(200), \n",
    "        'Decision tree - unlimited depth':cmap(250),\n",
    "        'Random forest':cmap(70), 'XGBoost':cmap(40)}\n",
    "\n",
    "get_template_roc_curve(ax,title='Receiver Operating Characteristic Curve\\nTest data',fs=15)\n",
    "    \n",
    "for classifier_name in classifiers_dictionary:\n",
    "    \n",
    "    model_and_predictions=fitted_models_and_predictions_dictionary[classifier_name]\n",
    "\n",
    "    FPR_list, TPR_list, threshold = metrics.roc_curve(test_df[output_feature], model_and_predictions['predictions_test'])\n",
    "    ROC_AUC = metrics.auc(FPR_list, TPR_list)\n",
    "\n",
    "    ax.plot(FPR_list, TPR_list, 'b', color=colors[classifier_name], label = 'AUC ROC {0}= {1:0.3f}'.format(classifier_name,ROC_AUC))\n",
    "    ax.legend(loc = 'upper left',bbox_to_anchor=(1.05, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81187468",
   "metadata": {},
   "source": [
    "We can clearly see that our Log Regression and our ROC Decision tree with a depth of 2 works far better than our other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3820a",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AP(precision, recall):\n",
    "    \n",
    "    AP = 0\n",
    "    \n",
    "    n_thresholds = len(precision)\n",
    "    \n",
    "    for i in range(1, n_thresholds):\n",
    "        \n",
    "        if recall[i]-recall[i-1]>=0:\n",
    "            \n",
    "            AP = AP+(recall[i]-recall[i-1])*precision[i]\n",
    "        \n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_pr_curve(ax, title,fs, baseline=0.5):\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "    \n",
    "    ax.set_xlabel('Recall (True Positive Rate)', fontsize=fs)\n",
    "    ax.set_ylabel('Precision', fontsize=fs)\n",
    "    \n",
    "    ax.plot([0, 1], [baseline, baseline],'r--',label='AP Random = {0:0.3f}'.format(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pr_curve, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors={'Logistic regression':cmap(0), 'Decision tree with depth of two':cmap(200), \n",
    "        'Decision tree - unlimited depth':cmap(250),\n",
    "        'Random forest':cmap(70), 'XGBoost':cmap(40)}\n",
    "\n",
    "get_template_pr_curve(ax, \"Precision Recall (PR) Curve\\nTest data\",fs=15,baseline=sum(test_df[output_feature])/len(test_df[output_feature]))\n",
    "    \n",
    "for classifier_name in classifiers_dictionary:\n",
    "    \n",
    "    model_and_predictions=fitted_models_and_predictions_dictionary[classifier_name]\n",
    "\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(test_df[output_feature], model_and_predictions['predictions_test'])\n",
    "    precision=precision[::-1]\n",
    "    recall=recall[::-1]\n",
    "    \n",
    "    AP = metrics.average_precision_score(test_df[output_feature], model_and_predictions['predictions_test'])\n",
    "    \n",
    "    ax.step(recall, precision, 'b', color=colors[classifier_name], label = 'AP {0}= {1:0.3f}'.format(classifier_name,AP))\n",
    "    ax.legend(loc = 'upper left',bbox_to_anchor=(1.05, 1))\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790878b",
   "metadata": {},
   "source": [
    "Our PR Curve gives us a different view of our classifiers. Compared to the ROC curve, these classifiers seem to be performing worse, due to the fact the the class imbalance problem is still addressed with PR performance metrics. The PR Curve is useful in highlighting the performance of fraud detection systems for low FPR values, but they still remain difficult to interpret from an operational point of view. When paired with the Precison top-k metrics, it becomes a little easier to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda4a3c",
   "metadata": {},
   "source": [
    "#### Precision top-k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "begin_date = '2016-06-25'\n",
    "end_date = '2016-07-25'\n",
    "\n",
    "%time df=read_from_files(DIR_OUTPUT, begin_date, end_date)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(df),df.isFraud.sum()))\n",
    "\n",
    "start_date_training = datetime.datetime.strptime(\"2016-06-25\", \"%Y-%m-%d\")\n",
    "delta_train=delta_delay=delta_test=7\n",
    "\n",
    "(train_df,test_df)=get_train_test_set(df,start_date_training,\n",
    "                                      delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "output_feature=\"isFraud\"\n",
    "\n",
    "input_features=['transactionAmount','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'merchantName_NB_TX_1DAY_WINDOW',\n",
    "       'merchantName_RISK_1DAY_WINDOW', 'merchantName_NB_TX_7DAY_WINDOW',\n",
    "       'merchantName_RISK_7DAY_WINDOW', 'merchantName_NB_TX_30DAY_WINDOW',\n",
    "       'merchantName_RISK_30DAY_WINDOW','cardPresent']\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=0)\n",
    "\n",
    "model_and_predictions_dictionary = fit_model_and_get_predictions(classifier, train_df, test_df, \n",
    "                                                                 input_features, output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_and_predictions_dictionary['predictions_test'])==len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df=test_df\n",
    "predictions_df['predictions']=model_and_predictions_dictionary['predictions_test']\n",
    "predictions_df[['ID','transactionDateTime','customerId','merchantName','transactionAmount','TX_Days','isFraud','predictions']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[predictions_df.isFraud==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_top_k(predictions_df, top_k=100):\n",
    "\n",
    "    # Sort days by increasing order\n",
    "    list_days=list(predictions_df['TX_Days'].unique())\n",
    "    list_days.sort()\n",
    "    \n",
    "    precision_top_k_per_day_list = []\n",
    "    nb_fraudulent_transactions_per_day = []\n",
    "    \n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "        \n",
    "        df_day = predictions_df[predictions_df['TX_Days']==day]\n",
    "        df_day = df_day[['ID', 'customerId', 'isFraud', 'predictions']]\n",
    "        \n",
    "        nb_fraudulent_transactions_per_day.append(len(df_day[df_day.isFraud==1]))\n",
    "        \n",
    "        _, _precision_top_k = precision_top_k_day(df_day, top_k=top_k)\n",
    "        \n",
    "        precision_top_k_per_day_list.append(_precision_top_k)\n",
    "        \n",
    "    # Compute the mean\n",
    "    mean_precision_top_k = np.round(np.array(precision_top_k_per_day_list).mean(),3)\n",
    "    \n",
    "    # Returns number of fraudulent transactions per day,\n",
    "    # precision top k per day, and resulting mean\n",
    "    return nb_fraudulent_transactions_per_day,precision_top_k_per_day_list,mean_precision_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d17ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_day(df_day,top_k):\n",
    "    \n",
    "    # This takes the max of the predictions AND the max of label TX_FRAUD for each CUSTOMER_ID, \n",
    "    # and sorts by decreasing order of fraudulent prediction\n",
    "    df_day = df_day.groupby('customerId').max().sort_values(by=\"predictions\", ascending=False).reset_index(drop=False)\n",
    "            \n",
    "    # Get the top k most suspicious cards\n",
    "    df_day_top_k=df_day.head(top_k)\n",
    "    list_detected_compromised_cards=list(df_day_top_k[df_day_top_k.isFraud==1].customerId)\n",
    "    \n",
    "    # Compute precision top k\n",
    "    card_precision_top_k = len(list_detected_compromised_cards) / top_k\n",
    "    \n",
    "    return list_detected_compromised_cards, card_precision_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad87d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fraudulent_transactions_per_day_remaining,\\\n",
    "precision_top_k_per_day_list,\\\n",
    "mean_precision_top_k = card_precision_top_k(predictions_df=predictions_df, top_k=100)\n",
    "\n",
    "print(\"Number of remaining fraudulent transactions: \"+str(nb_fraudulent_transactions_per_day_remaining))\n",
    "print(\"Precision top-k: \"+str(precision_top_k_per_day_list))\n",
    "print(\"Average Precision top-k: \"+str(mean_precision_top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffa4f4",
   "metadata": {},
   "source": [
    "We get an Average Precision Score of about 4.3% meaning out of the top 100 of the most suspicious transactions, about 4.3% were confirmed to be fraudulent. To better illustrate whats happenening with Precision K lets graph the number of fraudulent transactions, the number of remaining transactions for the test period, and the number of detected fraudulent transactions for the test period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b46171",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_stats=get_tx_stats(df, start_date_df=\"2016-01-01\")\n",
    "\n",
    "# Add the remaining number of fraudulent transactions for the last 7 days (test period)\n",
    "tx_stats.loc[14:20,'nb_fraudulent_transactions_per_day_remaining']=list(nb_fraudulent_transactions_per_day_remaining)\n",
    "# Add precision top k for the last 7 days (test period) \n",
    "tx_stats.loc[14:20,'precision_top_k_per_day']=precision_top_k_per_day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Plot the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors={'precision_top_k_per_day':cmap(0), \n",
    "        'nb_fraudulent_transactions_per_day':cmap(200),\n",
    "        'nb_fraudulent_transactions_per_day_remaining':cmap(250),\n",
    "       }\n",
    "\n",
    "fraud_and_transactions_stats_fig, ax = plt.subplots(1, 1, figsize=(15,8))\n",
    "\n",
    "# Training period\n",
    "start_date_training = datetime.datetime.strptime(\"2016-06-25\", \"%Y-%m-%d\")\n",
    "delta_train = delta_delay = delta_test = 7\n",
    "\n",
    "end_date_training = start_date_training+datetime.timedelta(days=delta_train-1)\n",
    "\n",
    "# Test period\n",
    "start_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay)\n",
    "end_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay+delta_test-1)\n",
    "\n",
    "get_template_tx_stats(ax, fs=20,\n",
    "                      start_date_training=start_date_training,\n",
    "                      title='Number of fraudulent transactions per day \\n and number of detected fraudulent transactions',\n",
    "                      delta_train=delta_train,\n",
    "                      delta_delay=delta_delay,\n",
    "                      delta_test=delta_test,\n",
    "                      ylim=150\n",
    "                     )\n",
    "\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_fraudulent_transactions_per_day'], 'b', color=colors['nb_fraudulent_transactions_per_day'], label = '# fraudulent txs per day - Original')\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_fraudulent_transactions_per_day_remaining'], 'b', color=colors['nb_fraudulent_transactions_per_day_remaining'], label = '# fraudulent txs per day - Remaining')\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['precision_top_k_per_day']*100, 'b', color=colors['precision_top_k_per_day'], label = '# detected fraudulent txs per day')\n",
    "ax.legend(loc = 'upper left',bbox_to_anchor=(1.05, 1),fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934153de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_and_transactions_stats_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fc57a",
   "metadata": {},
   "source": [
    "Lets pay special attention to the test period, which is what we used to really assess our Precision top k. We can see that in the testing period the number of total transactions varied between 30 and 50. Out of the remaining transactions left after taking out known compromised cards in the training period, we're left with about 10-25 transactions, and the fraud detecter we built was able to correctly detect about 1 to 10 or so transactions. Overall thats about less than 20% of actual fraudulent transactions that it has been able to detect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d823d",
   "metadata": {},
   "source": [
    "### Card Precision Top-K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0b2e7",
   "metadata": {},
   "source": [
    "This is similar to precision top k but instead of just looking flatly at transactions, we'll look at the amount of cards that have been compromised to see if our detector can correctly detect the amount of compromised cards out of the k cards which have the highest risk of frauds. We'll also plot it to get a visual idea of what is going on with our fraud detector system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k(predictions_df, top_k):\n",
    "\n",
    "    # Sort days by increasing order\n",
    "    list_days=list(predictions_df['TX_Days'].unique())\n",
    "    list_days.sort()\n",
    "    \n",
    "    card_precision_top_k_per_day_list = []\n",
    "    nb_compromised_cards_per_day = []\n",
    "    \n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "        \n",
    "        df_day = predictions_df[predictions_df['TX_Days']==day]\n",
    "        df_day = df_day[['predictions', 'customerId', 'isFraud']]\n",
    "        \n",
    "        nb_compromised_cards_per_day.append(len(df_day[df_day.isFraud==1].customerId.unique()))\n",
    "        \n",
    "        _, card_precision_top_k = card_precision_top_k_day(df_day,top_k)\n",
    "        \n",
    "        card_precision_top_k_per_day_list.append(card_precision_top_k)\n",
    "        \n",
    "    # Compute the mean\n",
    "    mean_card_precision_top_k = np.array(card_precision_top_k_per_day_list).mean()\n",
    "    \n",
    "    # Returns precision top k per day as a list, and resulting mean\n",
    "    return nb_compromised_cards_per_day,card_precision_top_k_per_day_list,mean_card_precision_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81725b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_compromised_cards_per_day_remaining\\\n",
    ",card_precision_top_k_per_day_list\\\n",
    ",mean_card_precision_top_k=card_precision_top_k(predictions_df=predictions_df, top_k=100)\n",
    "\n",
    "print(\"Number of remaining compromised cards: \"+str(nb_compromised_cards_per_day_remaining))\n",
    "print(\"Precision top-k: \"+str(card_precision_top_k_per_day_list))\n",
    "print(\"Average Precision top-k: \"+str(mean_card_precision_top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3cafa",
   "metadata": {},
   "source": [
    "As we can see our avg precision top k is about 0.055, meaning about 5.5% of fraudulent cards can be detected by our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of transactions per day, \n",
    "# fraudulent transactions per day and fraudulent cards per day\n",
    "tx_stats=get_tx_stats(df, start_date_df=\"2016-01-01\")\n",
    "\n",
    "# Add the remaining number of compromised cards for the last 7 days (test period)\n",
    "tx_stats.loc[14:20,'nb_compromised_cards_per_day_remaining']=list(nb_compromised_cards_per_day_remaining)\n",
    "\n",
    "# Add the card precision top k for the last 7 days (test period) \n",
    "tx_stats.loc[14:20,'card_precision_top_k_per_day']=card_precision_top_k_per_day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32456ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Plot the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors={'card_precision_top_k_per_day':cmap(0), \n",
    "        'nb_compromised_cards_per_day':cmap(200),\n",
    "        'nb_compromised_cards_per_day_remaining':cmap(250),\n",
    "       }\n",
    "\n",
    "fraud_and_transactions_stats_fig, ax = plt.subplots(1, 1, figsize=(15,8))\n",
    "\n",
    "# Training period\n",
    "start_date_training = datetime.datetime.strptime(\"2016-06-25\", \"%Y-%m-%d\")\n",
    "delta_train = delta_delay = delta_test = 7\n",
    "\n",
    "end_date_training = start_date_training+datetime.timedelta(days=delta_train-1)\n",
    "\n",
    "# Test period\n",
    "start_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay)\n",
    "end_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay+delta_test-1)\n",
    "\n",
    "get_template_tx_stats(ax, fs=20,\n",
    "                      start_date_training=start_date_training,\n",
    "                      title='Number of fraudulent transactions per day \\n and number of detected fraudulent transactions',\n",
    "                      delta_train=delta_train,\n",
    "                      delta_delay=delta_delay,\n",
    "                      delta_test=delta_test,\n",
    "                      ylim=150\n",
    "                     )\n",
    "\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_compromised_cards_per_day'], 'b', color=colors['nb_compromised_cards_per_day'], label = '# fraudulent txs per day - Original')\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['nb_compromised_cards_per_day_remaining'], 'b', color=colors['nb_compromised_cards_per_day_remaining'], label = '# compromised cards per day - Remaining')\n",
    "ax.plot(tx_stats['tx_date'], tx_stats['card_precision_top_k_per_day']*100, 'b', color=colors['card_precision_top_k_per_day'], label = '# detected compromised cards per day')\n",
    "ax.legend(loc = 'upper left', bbox_to_anchor=(1.05, 1), fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e06d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_and_transactions_stats_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f955fb",
   "metadata": {},
   "source": [
    "Very similar to our previous graph, we can see that when we take out the number of known compromised cards in the training period, we are left with about 10 to up to 25 cards that are compromised in the test period, and our system can detect about 2 to 8 or 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacba907",
   "metadata": {},
   "source": [
    "## Lets Build Our Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab9a64",
   "metadata": {},
   "source": [
    "Now that we have decided on our performace metrics and also have a decent overview of what models we should use, we will now build upon our previous knowledge to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We load more data than three weeks, as the experiments in the next sections\n",
    "# will require up to three months of data\n",
    "\n",
    "# Load data from the 2018-06-11 to the 2018-09-14\n",
    "\n",
    "DIR_OUTPUT = \"./pickled-data-raw/\"\n",
    "\n",
    "BEGIN_DATE = \"2016-06-25\"\n",
    "END_DATE = \"2016-11-25\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time df=read_from_files(DIR_OUTPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(df),df.isFraud.sum()))\n",
    "\n",
    "output_feature=\"isFraud\"\n",
    "\n",
    "input_features=['transactionAmount','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'merchantName_NB_TX_1DAY_WINDOW',\n",
    "       'merchantName_RISK_1DAY_WINDOW', 'merchantName_NB_TX_7DAY_WINDOW',\n",
    "       'merchantName_RISK_7DAY_WINDOW', 'merchantName_NB_TX_30DAY_WINDOW',\n",
    "       'merchantName_RISK_30DAY_WINDOW','cardPresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be243be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the starting day for the training period, and the deltas\n",
    "start_date_training = datetime.datetime.strptime(\"2018-06-25\", \"%Y-%m-%d\")\n",
    "delta_train=7\n",
    "delta_delay=7\n",
    "delta_test=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88205aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performances_train_test_sets(df, classifier,\n",
    "                                     input_features, output_feature,\n",
    "                                     start_date_training, \n",
    "                                     delta_train=7, delta_delay=7, delta_test=7,\n",
    "                                     top_k_list=[100],\n",
    "                                     type_test=\"Test\", parameter_summary=\"\"):\n",
    "\n",
    "    # Get the training and test sets\n",
    "    (train_df, test_df)=get_train_test_set(df,start_date_training,\n",
    "                                           delta_train=delta_train,\n",
    "                                           delta_delay=delta_delay,\n",
    "                                           delta_test=delta_test)\n",
    "    \n",
    "    # Fit model\n",
    "    start_time=time.time() \n",
    "    model_and_predictions_dictionary = fit_model_and_get_predictions(classifier, train_df, test_df, \n",
    "                                                                     input_features, output_feature)\n",
    "    execution_time=time.time()-start_time\n",
    "    \n",
    "    # Compute fraud detection performances\n",
    "    test_df['predictions']=model_and_predictions_dictionary['predictions_test']\n",
    "    performances_df_test=performance_assessment(test_df, top_k_list=top_k_list)\n",
    "    performances_df_test.columns=performances_df_test.columns.values+' '+type_test\n",
    "    \n",
    "    train_df['predictions']=model_and_predictions_dictionary['predictions_train']\n",
    "    performances_df_train=performance_assessment(train_df, top_k_list=top_k_list)\n",
    "    performances_df_train.columns=performances_df_train.columns.values+' Train'\n",
    "    \n",
    "    performances_df=pd.concat([performances_df_test,performances_df_train],axis=1)\n",
    "    \n",
    "    performances_df['Execution time']=execution_time\n",
    "    performances_df['Parameters summary']=parameter_summary\n",
    "    \n",
    "    return performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330faf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.tree.DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "performances_df=get_performances_train_test_sets(df, classifier, \n",
    "                                                 input_features, output_feature,\n",
    "                                                 start_date_training=start_date_training, \n",
    "                                                 delta_train=delta_train, \n",
    "                                                 delta_delay=delta_delay, \n",
    "                                                 delta_test=delta_test,\n",
    "                                                 parameter_summary=2\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149a089",
   "metadata": {},
   "source": [
    "### Prequential Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f57fe",
   "metadata": {},
   "source": [
    "As mentioned before, the point of building this fraud detection system is to maximize the detection of fraudulent transactions that will occur in the future. We introduced a delay period of 7 days to simulate how fraudulent transactions are usually discovered after a period of investigation. The model is trained on a set of past transactions, but the performance of a model on training data is often a bad indicator of the performance on future data. \n",
    "This due to **Overfitting**. Increasing the degree of freedom of a model (such as the depth of a decision tree) always allows for an increase in training performance, but this always leads to lower test performances. \n",
    "\n",
    "This can be solved with a step called **Validation**. Validation procedures aim to solve this issue by estimating on past data, the test performance of a prediction model by setting aside a prt of them. It splits past data into two or more sets, and play the role of the test set . The performance of a model on the validation set is used as an estimate of the performance that is expected on the test test. \n",
    "\n",
    "For this model we will use *Prequential Validation* which consists of using training sets of similar sizes, taken from older historical data.\n",
    "Each fold shifts the training and validation sets by one block in the past. So as time goes on our delay period till our test set will grow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe37ae",
   "metadata": {},
   "source": [
    "This function allows to create a custom function in sklearn for Card Precision top k. It is done by passing the transactions_df DataFrame as an argument into the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875588a",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "We will also employ the use of Grid Search, a tool in Sklearn that will alow us to fit and assess models with different [hyperparameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/). This will be done through the GridSearchCV funtion which main parameters are:\n",
    "\n",
    "1. estimator: The estimator to use, which will be a decision tree in the following example.\n",
    "2. param_grid: The set of hyperparameters for the estimator. We will vary the decision tree depth (max_depth) parameter, and set the random state (random_state) for reproduciblity.\n",
    "3. scoring: The scoring functions to use. We will use the AUC ROC, Average Precision, and CP@100.\n",
    "4. n_jobs: The number of cores to use. We will set it to -1, that is, to using all the available cores.\n",
    "5. refit: Whether the model should be fitted with all data after the cross validation. This will be set to false, since we only require the results of the cross validation.\n",
    "6. cv: The cross-validation splitting strategy. The prequential validation will be used, by passing the indices returned by the prequentialSplit function.\n",
    "\n",
    "We'll set our parameters and instantiate a GridSearchCV object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c84113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
